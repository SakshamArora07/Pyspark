{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4YoPFCyslaL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Random Forest Classification\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Assuming you have a DataFrame named 'data' containing your features and labels\n",
        "# data = ...\n",
        "\n",
        "# Create a vector assembler to assemble all feature columns into a single vector column\n",
        "vector_assembler = VectorAssembler(inputCols=data.columns[:-1], outputCol=\"features\")\n",
        "data_assembled = vector_assembler.transform(data)\n",
        "\n",
        "# Split the data into train and test sets (80% train, 20% test)\n",
        "(train_data, test_data) = data_assembled.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
        "\n",
        "# Train the model\n",
        "model = rf_classifier.fit(train_data)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate model performance\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ]
    }
  ]
}