{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we start off with importing necessary libraries:\n",
    "\n",
    "1.pyspark to build the application\n",
    "\n",
    "2.os for environment variables\n",
    "\n",
    "3.sys to interact with the python interpreter\n",
    "\n",
    "4.we import Sparkcontext which is the starting point to spark functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting up environment variables:\n",
    "os.environ['PYSPARK_PYTHON'/'PYSPARK_DRIVER_PYTHON']=sys.executable: This line sets the environment variable PYSPARK_PYTHON and 'PYSPARK_DRIVER_PYTHON' to the path of the Python executable currently running the script. This is done to ensure that PySpark, PySpark driver use the same python interpreter as the one running on script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkConext\n",
    "os.environ['PYSPARK_PYTHON']=sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession is a point of entry to interact with SparkSQL and dataframe APIs. \n",
    "We use the imported SparkSession to initialize a SparkSession named spark with a driver memory allocation of 16 GB and application name to 'chapter_8'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.config(\"spark.driver.memory\",\"16g\").appName('chapter_8').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we read data from 3 csv files specified in a list. the resulting dataframe contains combined data from all 3 files after their schemas are itnerpreted individually. it has a header contianing names of columns in csv as names of columns in dataframe. inferchema allows spark to autimatically infer schema used in the dataframe/csv files. \n",
    "\n",
    "stocks.show(2) shows first 2 rows from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = spark.read.csv([\"data/stocksA/ABAX.csv\",\"data/stocksA/AAME.csv\",\"data/stocksA/AEPI.csv\"], header='true', inferSchema='true')\n",
    "#stocks=spark.read.format(\"csv\").option(\"inferSchema\",\"true\").\n",
    "# option(\"header\",\"true\").load('C:/Users/HP/Desktop/aas-pyspark-edition/data/\n",
    "# stocksA/AAIT.csv').load('C:/Users/HP/Desktop/aas-pyspark-edition/data/stocksA/\n",
    "# AAME.csv')\n",
    "stocks.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the below cell, \n",
    "1. we first create a new column in the stocks dataframe called \"Symbol\" that contains the paths of the 3 input files. \n",
    "2. and then we process the file path to remove the directory name and retain only the file name by splitting valuesby '/' delimiter. (ex: result would be \"ABAX.csv\".)\n",
    "3. we further process it by splitting it again to only contain the file name without .csv (ex: result would be \"ABAX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "\n",
    "stocks = stocks.withColumn(\"Symbol\", fun.input_file_name()).\\\n",
    "withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"/\"), -1)).\\\n",
    "withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))\n",
    "\n",
    "stocks.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we repeat the reading of file and then adding symbol column, this time into a different dataframe called \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = spark.read.csv([\"data/stocksA/ABAX.csv\",\"data/stocksA/AAME.csv\",\"data/stocksA/AEPI.csv\"], header='true', inferSchema='true')\n",
    "\n",
    "factors = factors.withColumn(\"Symbol\", fun.input_file_name()).\\\n",
    "withColumn(\"Symbol\",\n",
    "fun.element_at(fun.split(\"Symbol\", \"/\"), -1)).\\\n",
    "withColumn(\"Symbol\",\n",
    "fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we count the occurrences of each unique value in the \"Symbol\" column. Then filter out rows where the count of occurrences for any symbol exceeds the threshold value of 260*5 + 10.\n",
    "\n",
    "we use window to partition the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "stocks = stocks.withColumn('count', fun.count('Symbol').\\\n",
    "over(Window.partitionBy('Symbol'))).\\\n",
    "filter(fun.col('count') > 260*5 + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we set the time parser policy to \"legacy\" which is different than the default parser to make it compatible with our requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we add a new column 'Date' that converts sting values of dates to date objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks.withColumn('Date',\n",
    "fun.to_date(fun.to_timestamp(fun.col('Date'),\n",
    "'dd-MMM-yy')))\n",
    "stocks.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we filter the values id date column to only contain values in the given range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "stocks = stocks.filter(fun.col('Date') >= datetime(2009, 10, 23)).\\\n",
    "filter(fun.col('Date') <= datetime(2014, 10, 23))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we repeat the above tow operations on factors DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = factors.withColumn('Date',\n",
    "fun.to_date(fun.to_timestamp(fun.col('Date'),\n",
    "'dd-MMM-yy')))\n",
    "factors = factors.filter(fun.col('Date') >= datetime(2009, 10, 23)).\\\n",
    "filter(fun.col('Date') <= datetime(2014, 10, 23))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we convert the spark dataframe into pandas dataframes to work on the data further using functions that are compatible with pandas DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_pd_df = stocks.toPandas()\n",
    "factors_pd_df = factors.toPandas()\n",
    "factors_pd_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, we calculate how much the price of a stock or factor changes over a period of 10 steps (like 10 days). This helps us understand how the value of the stock or factor changes over time.\n",
    "2. and then using this function, we calculate the rolling returns of each stock symbol and store them in two dataframes. \n",
    "3. we further use these values to create sequentil indices and then sort the DF based on values in 'level_1' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10\n",
    "def my_fun(x):\n",
    "    return ((x.iloc[-1] - x.iloc[0]) / x.iloc[0])\n",
    "\n",
    "stock_returns = stocks_pd_df.groupby('Symbol').Close.\\\n",
    "rolling(window=n_steps).apply(my_fun)\n",
    "\n",
    "factors_returns = factors_pd_df.groupby('Symbol').Close.\\\n",
    "rolling(window=n_steps).apply(my_fun)\n",
    "\n",
    "stock_returns = stock_returns.reset_index().\\\n",
    "sort_values('level_1').\\\n",
    "reset_index()\n",
    "\n",
    "factors_returns = factors_returns.reset_index().\\\n",
    "sort_values('level_1').\\\n",
    "reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we create a new DataFrame stocks_pd_df_with_returns by adding a new column called 'stock_returns' containing closing valse from the returns DF\n",
    "2. and another df containing closing values and squared returns to better represent returns and to capture any nonlinear relationships in rerurns.\n",
    "3. we re-arrange data to have index as date and symbol columns. we use \"pivot\" for a structured format of organization of data. \n",
    "4. we then rename columns of the df.we do this by concatinating the column names into a single string format separated by underscores, and then using this new string format as the column index of the DataFrame.\n",
    "5. we then rest index by converting current index to a column and generating a new sequential index starting from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined stocks DF\n",
    "stocks_pd_df_with_returns = stocks_pd_df.\\\n",
    "assign(stock_returns = \\\n",
    "stock_returns['Close'])\n",
    "# Create combined factors DF\n",
    "factors_pd_df_with_returns = factors_pd_df.\\\n",
    "assign(factors_returns = \\\n",
    "factors_returns['Close'],\n",
    "factors_returns_squared = \\\n",
    "factors_returns['Close']**2)\n",
    "\n",
    "factors_pd_df_with_returns = factors_pd_df_with_returns.\\\n",
    "pivot(index='Date',\n",
    "columns='Symbol',\n",
    "values=['factors_returns', \\\n",
    "'factors_returns_squared'])\n",
    "\n",
    "factors_pd_df_with_returns.columns = factors_pd_df_with_returns.\\\n",
    "columns.\\\n",
    "to_series().\\\n",
    "str.\\\n",
    "join('_').\\\n",
    "reset_index()[0]\n",
    "\n",
    "factors_pd_df_with_returns = factors_pd_df_with_returns.\\\n",
    "reset_index()\n",
    "\n",
    "print(factors_pd_df_with_returns.head(1)) #print one row of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print all colmns in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(factors_pd_df_with_returns.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now perform linear Regression using sklearn to find relationship between stock returns and factors. \n",
    "\n",
    "1. we first perform left join on the two returns DFs based on date\n",
    "2. and then we pre-process data to exclude missing or infinite values \n",
    "3. we then use the ML concept of oridanry least square to perform linear regression on our data and return a list of each stock symbol with the coefficients for each independent variable.\n",
    "4. we apply the ols function to our data and create a new DF contianing coefficients for each stock's regression model.\n",
    "5. we reset endex and rename columns\n",
    "6. lastly,  we create a new DF where each coefficient in the list becomes its own column. The index of the new DF remains the same as the original DF, and the columns are labeled with the stock symbol followed by the names of the feature columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# For each stock, create input DF for linear regression training\n",
    "stocks_factors_combined_df = pd.merge(stocks_pd_df_with_returns,\n",
    "factors_pd_df_with_returns,how=\"left\", on=\"Date\")\n",
    "\n",
    "feature_columns = list(stocks_factors_combined_df.columns[-6:])\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    stocks_factors_combined_df = stocks_factors_combined_df.\\\n",
    "    dropna(subset=feature_columns \\\n",
    "    + ['stock_returns'])\n",
    "    \n",
    "\n",
    "def find_ols_coef(df):\n",
    "    y = df[['stock_returns']].values\n",
    "    X = df[feature_columns]\n",
    "    regr = LinearRegression()\n",
    "    regr_output = regr.fit(X, y)\n",
    "    return list(df[['Symbol']].values[0]) + \\\n",
    "    list(regr_output.coef_[0])\n",
    "\n",
    "coefs_per_stock = stocks_factors_combined_df.\\\n",
    "groupby('Symbol').\\\n",
    "apply(find_ols_coef)\n",
    "\n",
    "coefs_per_stock = pd.DataFrame(coefs_per_stock).reset_index()\n",
    "coefs_per_stock.columns = ['symbol', 'factor_coef_list']\n",
    "\n",
    "coefs_per_stock = pd.DataFrame(coefs_per_stock.\\\n",
    "factor_coef_list.tolist(),\n",
    "index=coefs_per_stock.index,\n",
    "columns = ['Symbol'] + feature_columns)\n",
    "\n",
    "coefs_per_stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we thn plot a kernel density plot for the close prices of specific factors. This helps us udnerstand the distribution of prices for a specific factor. We can sue this distribution to look for anomalies or patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = factors_returns.loc[factors_returns.Symbol == \\\n",
    "factors_returns.Symbol.unique()[0]]['Close']\n",
    "samples.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create 3 series containing closing prices of 3 different symbols.\n",
    "we then print the size of each series to check if they are equal and then create a DF using them.\n",
    ".corr() calculates the correlation (measure of linear relationship) berween pairs of the variables. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1 = factors_returns.loc[factors_returns.Symbol == \\\n",
    "factors_returns.Symbol.unique()[0]]['Close']\n",
    "\n",
    "f_2 = factors_returns.loc[factors_returns.Symbol == \\\n",
    "factors_returns.Symbol.unique()[1]]['Close']\n",
    "\n",
    "f_3 = factors_returns.loc[factors_returns.Symbol == \\\n",
    "factors_returns.Symbol.unique()[2]]['Close']\n",
    "\n",
    "print(f_1.size,len(f_2),f_3.size)\n",
    "\n",
    "pd.DataFrame({'f1': list(f_1)[1:1040], 'f2': list(f_2)[1:1040], 'f3': \n",
    "list(f_3)}).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we computer covariance matrix (measure of how much two variables change together) and convert it to numpy array.\n",
    "2. and then we calculate the mean of closing prices for each symbol. \n",
    "\n",
    "these are done to obtain a mathematical insight into our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_returns_cov = pd.DataFrame({'f1': list(f_1)[1:1040],\n",
    "'f2': list(f_2)[1:1040],\n",
    "'f3': list(f_3)})\\\n",
    ".cov().to_numpy()\n",
    "\n",
    "factors_returns_mean = pd.DataFrame({'f1': list(f_1)[1:1040],\n",
    "'f2': list(f_2)[1:1040],\n",
    "'f3': list(f_3)}).\\\n",
    "mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a random sample of a normal distribution (of multivariate family) using the mean and covariance obtained above.  It produces a random vector that simulates the joint behavior of the factors based on their mean and covariance.\n",
    "we basically generate new/sample data in this step. we choose normal distribution based on KDE output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "multivariate_normal(factors_returns_mean, factors_returns_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then create braodcast variables to be shared across all nodes in the cluster. we broadcast coefs_per_stock,feature_columns,factors_returns_mean and factors_returns_cov to ALL nodes in our spark cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_coefs_per_stock = spark.sparkContext.broadcast(coefs_per_stock)\n",
    "b_feature_columns = spark.sparkContext.broadcast(feature_columns)\n",
    "b_factors_returns_mean = spark.sparkContext.broadcast(factors_returns_mean)\n",
    "b_factors_returns_cov = spark.sparkContext.broadcast(factors_returns_cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then generate random numbers across the cluster. this is done in parallel and each partition with have a unique seed for generating numbers. (so each partition will have the same generated numbers even on multiple runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "parallelism = 1000\n",
    "num_trials = 1000000\n",
    "base_seed = 1496\n",
    "seeds = [b for b in range(base_seed,\n",
    "base_seed + parallelism)]\n",
    "seedsDF = spark.createDataFrame(seeds, IntegerType())\n",
    "seedsDF = seedsDF.repartition(parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below code tests the outcomes of each stock under different circumstances using the coefficients from linear regression and the sample data generated parallely. The goal is to simulate the performance of each stock under various market conditions represented by the randomly generated factors. This helps us estimate how each stock's returns vary under various scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy.random import seed\n",
    "from pyspark.sql.types import LongType, ArrayType, DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "def calculate_trial_return(x):\n",
    "    # return x\n",
    "    trial_return_list = []\n",
    "    for i in range(int(num_trials/parallelism)):\n",
    "        random_int = random.randint(0, num_trials*num_trials)\n",
    "        seed(x)\n",
    "        random_factors = multivariate_normal(b_factors_returns_mean.value,\n",
    "        b_factors_returns_cov.value)\n",
    "        coefs_per_stock_df = b_coefs_per_stock.value\n",
    "        returns_per_stock = (coefs_per_stock_df[b_feature_columns.value] *\n",
    "        (list(random_factors) + list(random_factors**2)))\n",
    "    trial_return_list.append(float(returns_per_stock.sum(axis=1).sum()/\n",
    "    b_coefs_per_stock.value.size))\n",
    "    return trial_return_list\n",
    "udf_return = udf(calculate_trial_return, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. we calculate trial returns for each seed vale using UDF\n",
    "2. and then we flatten the trial_return column using \"explode\". this results in multiple rows for each seen value, each row representing a single trail return. \n",
    "3. finally we store the resultant DF using cache() for further analysis and faster access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "trials = seedsDF.withColumn(\"trial_return\", udf_return(col(\"value\")))\n",
    "trials = trials.select('value', explode('trial_return').alias('trial_return'))\n",
    "trials.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compute the return value of the bottom quartile (botton 5%) of all the simulated trials. This represents the worst 5% of all possible scenarios. this gives us an idea about the potental risks that come with our investments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.approxQuantile('trial_return', [0.05], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the below cell, we compute average trial returns in the bottom 5% to get an idea about the average losses/risks associated with the investment strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.orderBy(col('trial_return').asc()).\\\n",
    "limit(int(trials.count()/20)).\\\n",
    "agg(fun.avg(col(\"trial_return\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then visualize the reuslts obtained from above computation to get a visual idea about the extracted information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "mytrials=trials.toPandas()\n",
    "mytrials.plot.line()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
