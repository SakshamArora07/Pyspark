{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Prediction with Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stage 1: Data Preparation and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import os\n",
        "import sys\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "\n",
        "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_4').getOrCreate()\n",
        "\n",
        "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"data/covtype.data\")\n",
        "\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "colnames = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\",\n",
        "            \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
        "            \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n",
        "            \"Horizontal_Distance_To_Fire_Points\"] + \\\n",
        "            [f\"Wilderness_Area_{i}\" for i in range(4)] + \\\n",
        "            [f\"Soil_Type_{i}\" for i in range(40)] + \\\n",
        "            [\"Cover_Type\"]\n",
        "\n",
        "data = data_without_header.toDF(*colnames).withColumn(\"Cover_Type\", col(\"Cover_Type\").cast(DoubleType()))\n",
        "\n",
        "(train_data, test_data) = data.randomSplit([0.9, 0.1])\n",
        "train_data.cache()\n",
        "test_data.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stage 2: Feature Transformation and Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "input_cols = colnames[:-1]\n",
        "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
        "assembled_train_data = vector_assembler.transform(train_data)\n",
        "\n",
        "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
        "                                    featuresCol=\"featureVector\", predictionCol=\"prediction\")\n",
        "model = classifier.fit(assembled_train_data)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(model.featureImportances.toArray(), index=input_cols, columns=['importance']).sort_values(by=\"importance\", ascending=False)\n",
        "\n",
        "predictions = model.transform(assembled_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stage 3: Model Evaluation and Metrics Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\", predictionCol=\"prediction\")\n",
        "accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
        "f1_score = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "\n",
        "confusion_matrix = predictions.groupBy(\"Cover_Type\").pivot(\"prediction\", range(1, 8)).count().na.fill(0.0).orderBy(\"Cover_Type\")\n",
        "confusion_matrix.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stage 4: Pipeline Creation and Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "\n",
        "pipeline = Pipeline(stages=[vector_assembler, classifier])\n",
        "\n",
        "paramGrid = ParamGridBuilder().addGrid(classifier.impurity, [\"gini\", \"entropy\"]).addGrid(classifier.maxDepth, [1, 20]).addGrid(classifier.maxBins, [40, 300]).addGrid(classifier.minInfoGain, [0.0, 0.05]).build()\n",
        "\n",
        "multiclassEval = MulticlassClassificationEvaluator().setLabelCol(\"Cover_Type\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
        "\n",
        "validator = TrainValidationSplit(seed=1234, estimator=pipeline, evaluator=multiclassEval,\n",
        "                                  estimatorParamMaps=paramGrid, trainRatio=0.9)\n",
        "\n",
        "validator_model = validator.fit(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stage 5: Model Selection and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "best_model = validator_model.bestModel\n",
        "best_model.stages[1].extractParamMap()\n",
        "\n",
        "metrics_and_params.sort(key=lambda x: x[0], reverse=True)\n",
        "metrics.sort(reverse=True)\n",
        "print(metrics[0])\n",
        "\n",
        "multiclassEval.evaluate(best_model.transform(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stage 6: Data Transformation for One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def unencode_one_hot(data):\n",
        "    # Function to unencode one-hot encoded columns for Wilderness Area and Soil Type\n",
        "    return processed_data\n",
        "\n",
        "unenc_train_data = unencode_one_hot(train_data)\n",
        "unenc_train_data.printSchema()\n",
        "unenc_train_data.groupBy('wilderness').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stage 7: Random Forest Classifier with Pipeline and Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "classifier_rf = RandomForestClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
        "                                       featuresCol=\"indexedVector\", predictionCol=\"prediction\")\n",
        "\n",
        "pipeline_rf = Pipeline().setStages([assembler, indexer, classifier_rf])\n",
        "\n",
        "validator_model_rf = validator.fit(unenc_train_data)\n",
        "\n",
        "best_model_rf = validator_model_rf.bestModel.stages[2]\n",
        "\n",
        "feature_importance_list.sort(key=lambda x: x[1], reverse=True)\n",
        "pprint(feature_importance_list)\n",
        "\n",
        "unenc_test_data = unencode_one_hot(test_data)\n",
        "best_model_rf.transform(unenc_test_data.drop(\"Cover_Type\")).select(\"prediction\").show(1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
