{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection with K-means Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Exercises:\n",
    "1. Implement a PySpark script to handle any missing values and scale numerical features.\n",
    "2. Develop a PySpark script that uses the K-means algorithm to cluster data points.\n",
    "3. Develop a PySpark script that labels data points as anomalies based on their cluster assignments.\n",
    "4. Implement code to evaluate the effectiveness of the K-means clustering model in detecting anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jinp1eY5GLlp"
   },
   "source": [
    "**Importing necessary modules and setting environment variables for PySpark to use the correct Python executable. Importing `SparkContext` from `pyspark` and `SparkSession` from `pyspark.sql`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VymElEEquWig"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8PkYG9PGNMU"
   },
   "source": [
    "**Initializing a SparkSession named 'chapter_5' with a driver memory of 16 GB using the `SparkSession.builder.config()` method.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sw0q03RNueyA"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_5').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XYmUoUPujth"
   },
   "source": [
    "## A First Take on Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkY2b0npGQY1"
   },
   "source": [
    "**Reading a CSV file located at \"data/kddcup.data_10_percent_corrected\" into a Spark DataFrame `data_without_header`, treating the first row as data rather than a header. Then assigning column names to the DataFrame based on a predefined list `column_names`. Finally, creating a new DataFrame `data` with the specified column names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jk-1no3SugYu"
   },
   "outputs": [],
   "source": [
    "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"data/kddcup.data_10_percent_corrected\")\n",
    "\n",
    "column_names = [ \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "  \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "  \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "  \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "  \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "  \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "  \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "  \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "  \"dst_host_count\", \"dst_host_srv_count\",\n",
    "  \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "  \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "  \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "  \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "  \"label\"]\n",
    "\n",
    "data = data_without_header.toDF(*column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vre3f-QrGVUX"
   },
   "source": [
    "**Selecting the 'label' column from the DataFrame `data`, grouping it by unique values in the 'label' column, counting the occurrences of each label, ordering the result by the count in descending order, and displaying the top 25 labels along with their counts.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZNT2ladujON"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaFOXELLGYys"
   },
   "source": [
    "**Importing necessary modules and classes from PySpark for performing K-means clustering. First dropping non-numeric columns from the DataFrame `data` and caching the resulting DataFrame `numeric_only`. Then, setting up a `VectorAssembler` to assemble feature vectors from numeric columns, followed by initializing a K-means model. A pipeline is constructed to execute these transformations and modeling steps sequentially. After fitting the pipeline to the data, the cluster centers from the trained K-means model are printed using the `clusterCenters()` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IidDPMgbupRf"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "\n",
    "assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "\n",
    "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipeline_model = pipeline.fit(numeric_only)\n",
    "kmeans_model = pipeline_model.stages[1]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(kmeans_model.clusterCenters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcDABEbOGcvi"
   },
   "source": [
    "**Applying the trained pipeline model `pipeline_model` to the `numeric_only` DataFrame, generating predictions and adding a 'cluster' column to the DataFrame `with_cluster`. Then selecting the 'cluster' and 'label' columns, groups by both columns, counting the occurrences of each combination, ordering the result first by cluster and then by count in descending order, and displaying the top 25 combinations of clusters and labels along with their counts.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OW85BTJQurz_"
   },
   "outputs": [],
   "source": [
    "with_cluster = pipeline_model.transform(numeric_only)\n",
    "\n",
    "with_cluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MQiaa6HuuRw"
   },
   "source": [
    "## Choosing k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-UnDeDFGggx"
   },
   "source": [
    "**Defining a function `clustering_score(input_data, k)` to calculate the training cost of K-means clustering for a given value of `k` clusters. Within the function, dropping non-numeric columns, setting up a pipeline with a `VectorAssembler` and a K-means model, fitting the pipeline to the data, and retrieving the training cost from the K-means model summary.**\n",
    "\n",
    "**Then, iterating over a range of `k` values from 20 to 80 (inclusive), incrementing by 20, and printing the training cost for each `k`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4puBGqKuvfS"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from random import randint\n",
    "\n",
    "def clustering_score(input_data, k):\n",
    "  input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "  assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "  kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "  pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "  pipeline_model = pipeline.fit(input_numeric_only)\n",
    "  kmeans_model = pipeline_model.stages[-1]\n",
    "  training_cost = kmeans_model.summary.trainingCost\n",
    "  return training_cost\n",
    "\n",
    "for k in list(range(20,100, 20)):\n",
    "  print(clustering_score(numeric_only, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaitmkRaGkCV"
   },
   "source": [
    "**Defining a modified version of the `clustering_score` function called `clustering_score_1`. Calculating the training cost of K-means clustering for a given value of `k` clusters. Within the function, dropping non-numeric columns, setting up a pipeline with a `VectorAssembler` and a K-means model with additional parameters (`setMaxIter`, `setTol`), and fitting the pipeline to the data.**\n",
    "\n",
    "**Then, iterating over a range of `k` values from 20 to 100 (inclusive), incrementing by 20, and printing the `k` value along with the corresponding training cost calculated by the `clustering_score_1` function.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK7PB96hu0vB"
   },
   "outputs": [],
   "source": [
    "def clustering_score_1(input_data, k):\n",
    "  input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "  assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "  kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "  pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "  pipeline_model = pipeline.fit(input_numeric_only)\n",
    "  kmeans_model = pipeline_model.stages[-1]\n",
    "  training_cost = kmeans_model.summary.trainingCost\n",
    "  return training_cost\n",
    "\n",
    "for k in list(range(20,101, 20)):\n",
    "  print(k, clustering_score_1(numeric_only, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKDLp_Y2u8Sl"
   },
   "source": [
    "## Feature Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcbXTfm-Gnpq"
   },
   "source": [
    "**Defining another modified version of the `clustering_score` function called `clustering_score_2`. Calculating the training cost of K-means clustering for a given value of `k` clusters. Within the function, dropping non-numeric columns, setting up a pipeline with a `VectorAssembler`, a `StandardScaler`, and a K-means model with additional parameters (`setMaxIter`, `setTol`), and fitting the pipeline to the data.**\n",
    "\n",
    "**Then, iterating over a range of `k` values from 60 to 270 (inclusive), incrementing by 30, and printing the `k` value along with the corresponding training cost calculated by the `clustering_score_2` function.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMkNmsNqu9kq"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "def clustering_score_2(input_data, k):\n",
    "  input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "  assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "  scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
    "  kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
    "  pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "  pipeline_model = pipeline.fit(input_numeric_only)\n",
    "  kmeans_model = pipeline_model.stages[-1]\n",
    "  training_cost = kmeans_model.summary.trainingCost\n",
    "  return training_cost\n",
    "\n",
    "for k in list(range(60, 271, 30)):\n",
    "  print(k, clustering_score_2(numeric_only, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgMNVDw-vFkp"
   },
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQHAqaQ-Gry5"
   },
   "source": [
    "**Defining a function `one_hot_pipeline(input_col)` to create a pipeline for one-hot encoding a categorical column. Within the function, setting up a `StringIndexer` to index the categorical column and a `OneHotEncoder` to encode the indexed column. These transformations are encapsulated in a pipeline.**\n",
    "\n",
    "**The function returns the pipeline and the name of the output column after one-hot encoding.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1U26-ShvGpq"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "def one_hot_pipeline(input_col):\n",
    "  indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col + \"_indexed\")\n",
    "  encoder = OneHotEncoder().setInputCol(input_col + \"_indexed\").setOutputCol(input_col + \"_vec\")\n",
    "  pipeline = Pipeline().setStages([indexer, encoder])\n",
    "  return pipeline, input_col + \"_vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfETwcu7GvE0"
   },
   "source": [
    "**Defining the function `clustering_score_3(input_data, k)` to calculate the training cost of K-means clustering for a given value of `k` clusters, considering one-hot encoded categorical features. Within the function, setting pipelines for one-hot encoding of categorical columns (\"protocol_type\", \"service\", \"flag\"), assembling all relevant columns, applying standard scaling, and training a K-means model.**\n",
    "\n",
    "**Then, iterating over a range of `k` values from 60 to 270 (inclusive), incrementing by 30, and printing the `k` value along with the corresponding training cost calculated by the `clustering_score_3` function.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PYT2zS6vM3m"
   },
   "outputs": [],
   "source": [
    "def clustering_score_3(input_data, k):\n",
    "  proto_type_pipeline, proto_type_vec_col = one_hot_pipeline(\"protocol_type\")\n",
    "  service_pipeline, service_vec_col = one_hot_pipeline(\"service\")\n",
    "  flag_pipeline, flag_vec_col = one_hot_pipeline(\"flag\")\n",
    "\n",
    "  assemble_cols = set(input_data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "\n",
    "  assembler = VectorAssembler().setInputCols(list(assemble_cols)).setOutputCol(\"featureVector\")\n",
    "  scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
    "  kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n",
    "  pipeline = Pipeline().setStages([proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n",
    "  pipeline_model = pipeline.fit(input_data)\n",
    "\n",
    "  kmeans_model = pipeline_model.stages[-1]\n",
    "  training_cost = kmeans_model.summary.trainingCost\n",
    "  return training_cost\n",
    "\n",
    "for k in list(range(60, 271, 30)):\n",
    "  print(k, clustering_score_3(data, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_BFiGD6vUBO"
   },
   "source": [
    "## Using Labels with Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoPm_uHrGyr-"
   },
   "source": [
    "**Defining a function `entropy(counts)` to calculate the entropy of a list of counts. Within the function, calculating the probability of each count, computing the entropy using the formula `-p * log(p)`, and suming the results. The function returns the entropy value.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf99H0_JvQYS"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def entropy(counts):\n",
    "  values = [c for c in counts if (c > 0)]\n",
    "  n = sum(values)\n",
    "  p = [v/n for v in values]\n",
    "  return sum([-1*(p_v) * log(p_v) for p_v in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgce4T3SG3S5"
   },
   "source": [
    "**Computing the weighted average of cluster entropy for the provided data. First transforming the data using the trained pipeline model to obtain cluster labels. Then, calculating the count of each label within each cluster and ordering the result by cluster. After that, computing the probability of each label count within each cluster and adding a new column for the probability.**\n",
    "\n",
    "**Next, calculating the entropy for each cluster by summing the product of the probability and the log probability for each label count within the cluster. Then, computing the weighted cluster entropy by multiplying the entropy with the cluster size.**\n",
    "\n",
    "**Finally, aggregating the weighted cluster entropy across all clusters and dividing it by the total number of data points to get the weighted average cluster entropy.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDfGXqkQvWF2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import Window\n",
    "\n",
    "cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\")\n",
    "\n",
    "df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
    "\n",
    "w = Window.partitionBy(\"cluster\")\n",
    "\n",
    "p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "\n",
    "result = with_p_col.groupBy(\"cluster\").agg((-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\")))).alias(\"entropy\"),fun.sum(col(\"count\")).alias(\"cluster_size\"))\n",
    "\n",
    "result = result.withColumn('weightedClusterEntropy',fun.col('entropy') * fun.col('cluster_size'))\n",
    "\n",
    "weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n",
    "weighted_cluster_entropy_avg[0][0]/data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMWTphFHG6pu"
   },
   "source": [
    "**Defining a function `fit_pipeline_4(data, k)` to fit a pipeline for K-means clustering with one-hot encoding of categorical features and standard scaling. First setting up pipelines for one-hot encoding of categorical columns (\"protocol_type\", \"service\", \"flag\"). Then, assembling relevant columns, applies standard scaling, and training a K-means model using the specified number of clusters (`k`).**\n",
    "\n",
    "**Another function `clustering_score_4(input_data, k)` is defined to calculate the weighted average cluster entropy for a given dataset and number of clusters. Within the function, fitting the pipeline to the input data, transforming the data to obtain cluster labels, calculating the cluster entropy, and computing the weighted average cluster entropy.**\n",
    "\n",
    "**Both functions are designed to enhance modularity and readability, making it easier to fit the pipeline and calculate the clustering score.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J56V_FyqvZqq"
   },
   "outputs": [],
   "source": [
    "def fit_pipeline_4(data, k):\n",
    "  (proto_type_pipeline, proto_type_vec_col) = one_hot_pipeline(\"protocol_type\")\n",
    "  (service_pipeline, service_vec_col) = one_hot_pipeline(\"service\")\n",
    "  (flag_pipeline, flag_vec_col) = one_hot_pipeline(\"flag\")\n",
    "  assemble_cols = set(data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "  assembler = VectorAssembler(inputCols=list(assemble_cols), outputCol=\"featureVector\")\n",
    "\n",
    "  scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
    "\n",
    "  kmeans = KMeans(seed=randint(100, 100000), k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-5)\n",
    "\n",
    "  pipeline = Pipeline(stages=[proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n",
    "  return pipeline.fit(data)\n",
    "\n",
    "def clustering_score_4(input_data, k):\n",
    "  pipeline_model = fit_pipeline_4(input_data, k)\n",
    "  cluster_label = pipeline_model.transform(input_data).select(\"cluster\", \"label\")\n",
    "\n",
    "  df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
    "\n",
    "  w = Window.partitionBy(\"cluster\")\n",
    "\n",
    "  p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "  with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "\n",
    "  result = with_p_col.groupBy(\"cluster\").agg(-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\"))).alias(\"entropy\"),fun.sum(col(\"count\")).alias(\"cluster_size\"))\n",
    "\n",
    "  result = result.withColumn('weightedClusterEntropy', col('entropy') * col('cluster_size'))\n",
    "\n",
    "  weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n",
    "  return weighted_cluster_entropy_avg[0][0] / input_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ8Q12BHvjRm"
   },
   "source": [
    "## Clustering in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jovUceeaG_hR"
   },
   "source": [
    "**Fitting the pipeline model for K-means clustering with 180 clusters to the provided data and then applying the trained model to the data to obtain cluster labels. Selecting the 'cluster' and 'label' columns, grouping them by cluster and label, calculating the count of each combination, and ordering the result by cluster and label. Finally, displaying the resulting DataFrame showing the counts of each label within each cluster.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrCUQ2f_vkdz"
   },
   "outputs": [],
   "source": [
    "pipeline_model = fit_pipeline_4(data, 180)\n",
    "count_by_cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\", \"label\")\n",
    "count_by_cluster_label.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMcfcJfAtNZ9vM6HeICtu8V",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
