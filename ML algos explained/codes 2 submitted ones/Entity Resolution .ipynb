{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTITY RESOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrvUusK3JIL4"
   },
   "source": [
    "### Description\n",
    "\n",
    "This code initializes a PySpark environment for distributed data processing. It sets up necessary configurations and imports required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IerCYfMkJHlS"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaySZ8IsJSzm"
   },
   "source": [
    "## Configuring Spark Session\n",
    "\n",
    "Configuring a SparkSession named 'chapter_2' with specific driver memory settings.\n",
    "\n",
    "- **Configuring Driver Memory**: `.config(\"spark.driver.memory\", \"16g\")` sets the driver memory to 16 gigabytes.\n",
    "- **Creating SparkSession**: `SparkSession.builder.appName('chapter_2').getOrCreate()` creates a SparkSession with the specified configuration and application name.\n",
    "\n",
    "Configuring the SparkSession with appropriate memory settings is crucial for managing resources effectively and optimizing performance, especially for memory-intensive tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zoo1r1czJe5k"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_2').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data and analyzing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXHk9Rd6Jgei"
   },
   "source": [
    "## Reading CSV Data\n",
    "\n",
    "Reading CSV data from the specified file path using SparkSession.\n",
    "\n",
    "- **CSV File Path**: The CSV data is read from the file path `\"data/linkage/donation/block_1/block_1.csv\"`.\n",
    "- **Using SparkSession**: `spark.read.csv()` is used to read CSV files into a DataFrame.\n",
    "- **DataFrame Creation**: The resulting DataFrame contains the data from the CSV file.\n",
    "\n",
    "This operation loads the data from the CSV file into a DataFrame, allowing for further analysis and processing using Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIQ3aZUMJjRO"
   },
   "outputs": [],
   "source": [
    "prev = spark.read.csv(\"data/linkage/donation/block_1/block_1.csv\")\n",
    "\n",
    "prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tR1wSbffJ1QU"
   },
   "source": [
    "## Displaying Data\n",
    "\n",
    "Displaying the first two rows of the DataFrame `prev`.\n",
    "\n",
    "- **DataFrame Display**: `prev.show(2)` is used to show the first two rows of the DataFrame.\n",
    "\n",
    "Displaying a subset of the data provides an overview of its structure and content, facilitating further exploration and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbqDLTBQJ9sV"
   },
   "outputs": [],
   "source": [
    "prev.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4f-yQMYKADA"
   },
   "source": [
    "## Reading CSV Data with Parsing Options\n",
    "\n",
    "Reading CSV data from the specified file path using SparkSession and provides parsing options.\n",
    "\n",
    "- **CSV File Path**: The CSV data is read from the file path `\"data/linkage/donation/block_1/block_1.csv\"`.\n",
    "- **Parsing Options**:\n",
    "  - `header=\"true\"`: Specifies that the first row contains the column names.\n",
    "  - `nullValue=\"?\"`: Specifies that \"?\" is considered as null value during parsing.\n",
    "  - `inferSchema=\"true\"`: Enables automatic schema inference based on data types in the CSV file.\n",
    "- **DataFrame Creation**: The resulting DataFrame contains the parsed data from the CSV file.\n",
    "\n",
    "Using parsing options ensures accurate interpretation of the CSV data, including handling of headers, null values, and automatic schema inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pEwnIncKHzN"
   },
   "outputs": [],
   "source": [
    "parsed = spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").option(\"inferSchema\", \"true\").csv(\"data/linkage/donation/block_1/block_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTGyhH-qKLRu"
   },
   "source": [
    "## Analyzing Data with the DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVSUib-XKOFk"
   },
   "source": [
    "## Printing Schema and Displaying Data\n",
    "\n",
    "Printing the schema and displays the first five rows of the DataFrame `parsed`.\n",
    "\n",
    "- **Printing Schema**: `parsed.printSchema()` prints the schema of the DataFrame.\n",
    "- **Displaying Data**: `parsed.show(5)` displays the first five rows of the DataFrame.\n",
    "\n",
    "Printing the schema helps in understanding the structure of the DataFrame, while displaying data provides an initial glimpse into its contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTm3srK6KNiN"
   },
   "outputs": [],
   "source": [
    "parsed.printSchema()\n",
    "\n",
    "parsed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYUG8M3HKVdk"
   },
   "source": [
    "## Counting Rows\n",
    "\n",
    "Calculating the number of rows in the DataFrame `parsed`.\n",
    "\n",
    "- **Counting Rows**: `parsed.count()` computes the total number of rows in the DataFrame.\n",
    "\n",
    "Counting rows provides an overview of the dataset's size, helping to gauge its scale and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTPGOknQKcW3"
   },
   "outputs": [],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xO7NfHvrKe5N"
   },
   "source": [
    "## Caching DataFrame\n",
    "\n",
    "Caches the DataFrame `parsed` in memory for faster access.\n",
    "\n",
    "- **Caching Data**: `parsed.cache()` caches the DataFrame in memory.\n",
    "\n",
    "Caching is particularly useful for iterative operations or when you need to reuse the DataFrame multiple times within the same computation, as it avoids reevaluation of DataFrame transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hoTC266Khny"
   },
   "outputs": [],
   "source": [
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC7ce4hkKkJD"
   },
   "source": [
    "## Grouping and Aggregating Data\n",
    "\n",
    "Grouping the DataFrame `parsed` by the column \"is_match\" and aggregates the counts of each group, then orders the result by count in descending order.\n",
    "\n",
    "- **Grouping and Aggregating**: `parsed.groupBy(\"is_match\").count()` groups the data by the \"is_match\" column and calculates the count of each group.\n",
    "- **Sorting**: `.orderBy(col(\"count\").desc())` sorts the aggregated data in descending order based on the count.\n",
    "- **Displaying Results**: `.show()` displays the grouped and aggregated data.\n",
    "\n",
    "This analysis provides insights into the distribution of matches and non-matches in the dataset, which can be crucial for understanding the data characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nNJQ_hgKkSf"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpdTHNeftHXn"
   },
   "source": [
    "## Creating Temporary View\n",
    "\n",
    "Creating a temporary view named \"linkage\" for the DataFrame `parsed`.\n",
    "\n",
    "- **Creating Temporary View**: `parsed.createOrReplaceTempView(\"linkage\")` creates a temporary view in the Spark SQL context.\n",
    "\n",
    "Creating a temporary view allows you to query the DataFrame using SQL syntax, enabling more flexible and expressive data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZUSQHLmrxMZ"
   },
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute SQL query to group by 'is_match' and count occurrences, then order by count in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrjBmkiatJUv"
   },
   "source": [
    "## Executing SQL Query\n",
    "\n",
    "Executing an SQL query using Spark SQL.\n",
    "\n",
    "- **SQL Query**: The SQL query selects the \"is_match\" column and counts the occurrences of each value, grouping by \"is_match\" and ordering the results by count in descending order.\n",
    "- **Execution**: `spark.sql(\"\"\" ... \"\"\")` executes the SQL query using Spark SQL.\n",
    "- **Displaying Results**: `.show()` displays the results of the SQL query.\n",
    "\n",
    "Using SQL queries with Spark SQL provides a familiar and powerful interface for data analysis, especially for users comfortable with SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRYl25yXrz62"
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SELECT is_match, COUNT(*) cnt\n",
    "  FROM linkage\n",
    "  GROUP BY is_match\n",
    "  ORDER BY cnt DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Summary Statistics, Plotting and Reshaping DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2v1YqA7r5h_"
   },
   "source": [
    "## Fast Summary Statistics for DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U_HC6jSty3L"
   },
   "source": [
    "**Generating a summary statistics DataFrame `summary` by describing the parsed DataFrame `parsed`. It computes basic statistics for each numerical column in the DataFrame, including count, mean, standard deviation, min, max, and quartiles.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmaPWZDsr4gJ"
   },
   "outputs": [],
   "source": [
    "summary = parsed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvWY3oHUH2K-"
   },
   "source": [
    "**Selecting specific columns ('summary', 'cmp_fname_c1', 'cmp_fname_c2') from the `summary` DataFrame and displays the result. It likely shows summary statistics related to the columns 'cmp_fname_c1' and 'cmp_fname_c2'.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWgAdiR9r9PE"
   },
   "outputs": [],
   "source": [
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_DBJeJjH3L-"
   },
   "source": [
    "**Filtering the `parsed` DataFrame into two separate DataFrames: `matches`, which contains rows where the 'is_match' column is true, and `misses`, which contains rows where the 'is_match' column is false. Then, it computes summary statistics for both DataFrames (`match_summary` and `miss_summary`) using the `describe()` function.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiuGaq9mr-3N"
   },
   "outputs": [],
   "source": [
    "matches = parsed.where(\"is_match = true\")\n",
    "match_summary = matches.describe()\n",
    "\n",
    "misses = parsed.filter(col(\"is_match\") == False)\n",
    "miss_summary = misses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGrSOR0wsAXP"
   },
   "source": [
    "## Pivoting and Reshaping DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APLGGrz1H6oF"
   },
   "source": [
    "**Converting the Spark DataFrame `summary` to a Pandas DataFrame named `summary_p`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1IONXRcsBpQ"
   },
   "outputs": [],
   "source": [
    "summary_p = summary.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8PXXiTJH9S0"
   },
   "source": [
    "**Displaying the first few rows of the Pandas DataFrame `summary_p` using the `head()` method and outputs the shape of the DataFrame using the `shape` attribute.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gX7fOc3sEtE"
   },
   "outputs": [],
   "source": [
    "summary_p.head()\n",
    "\n",
    "summary_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aajhflrOIAEi"
   },
   "source": [
    "**Transforming the Pandas DataFrame `summary_p` by setting the 'summary' column as the index, transposing the DataFrame, resetting the index, and renaming the columns appropriately. Finally, it removes the axis name to clean up the DataFrame. The code then outputs the shape of the transformed DataFrame.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh4GrsZ4sFed"
   },
   "outputs": [],
   "source": [
    "summary_p = summary_p.set_index('summary').transpose().reset_index()\n",
    "\n",
    "summary_p = summary_p.rename(columns={'index':'field'})\n",
    "\n",
    "summary_p = summary_p.rename_axis(None, axis=1)\n",
    "\n",
    "summary_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHP3JY7YIEin"
   },
   "source": [
    "**Creating a Spark DataFrame named `summaryT` from the Pandas DataFrame `summary_p`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8wI3-bjsHwa"
   },
   "outputs": [],
   "source": [
    "summaryT = spark.createDataFrame(summary_p)\n",
    "\n",
    "summaryT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAIZVvaAIHs7"
   },
   "source": [
    "**Printing the schema of the Spark DataFrame `summaryT`, displaying the data types and nullable properties of each column.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxihTtkrsJQk"
   },
   "outputs": [],
   "source": [
    " summaryT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6G0vL-HCIMBV"
   },
   "source": [
    "**Iterating through the columns of the Spark DataFrame `summaryT` and converts all columns except 'field' to DoubleType using the `cast()` method. Afterwards, it prints the schema of the updated DataFrame to reflect the changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTPy3Qj_sLbE"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "for c in summaryT.columns:\n",
    "  if c == 'field':\n",
    "    continue\n",
    "  summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))\n",
    "\n",
    "summaryT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Agqkcn_HIPXU"
   },
   "source": [
    "**Defining a function `pivot_summary(desc)` to pivot summary statistics DataFrame. It converts the input DataFrame `desc` to a Pandas DataFrame, transposes it, resets the index, renames columns, and converts it back to a Spark DataFrame. Then, it converts metric columns to DoubleType from String in the Spark DataFrame. Finally, it returns the transformed Spark DataFrame.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjUS8qYrsNdb"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def pivot_summary(desc):\n",
    "  # convert to pandas dataframe\n",
    "  desc_p = desc.toPandas()\n",
    "  # transpose\n",
    "  desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "  desc_p = desc_p.rename(columns={'index':'field'})\n",
    "  desc_p = desc_p.rename_axis(None, axis=1)\n",
    "  # convert to Spark dataframe\n",
    "  descT = spark.createDataFrame(desc_p)\n",
    "  # convert metric columns to double from string\n",
    "  for c in descT.columns:\n",
    "    if c == 'field':\n",
    "      continue\n",
    "    else:\n",
    "      descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "    return descT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM1uUvskISfI"
   },
   "source": [
    "**This code applies the `pivot_summary()` function to the `match_summary` and `miss_summary` DataFrames, resulting in `match_summaryT` and `miss_summaryT` Spark DataFrames with pivoted summary statistics.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prd0-pdosVjY"
   },
   "outputs": [],
   "source": [
    "match_summaryT = pivot_summary(match_summary)\n",
    "miss_summaryT = pivot_summary(miss_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW-v6f5nsaa4"
   },
   "source": [
    "## Joining DataFrames and Selecting Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W05DxSJ4IV2M"
   },
   "source": [
    "**This code creates temporary views \"match_desc\" and \"miss_desc\" for the Spark DataFrames `match_summaryT` and `miss_summaryT`, respectively. Then, it executes a SQL query to join these views on the 'field' column and selects fields that are not \"id_1\" or \"id_2\". It calculates the total count for each field by adding the counts from both match and miss dataframes and computes the difference in mean between match and miss dataframes. Finally, it orders the result by delta (difference in mean) in descending order, and then by total count in descending order.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOPauZASsYEn"
   },
   "outputs": [],
   "source": [
    "match_summaryT.createOrReplaceTempView(\"match_desc\")\n",
    "miss_summaryT.createOrReplaceTempView(\"miss_desc\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT a.field, a.count + b.count total, a.mean - b.mean delta\n",
    "  FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field\n",
    "  WHERE a.field NOT IN (\"id_1\", \"id_2\")\n",
    "  ORDER BY delta DESC, total DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GWk9dDjsc8c"
   },
   "source": [
    "## Scoring and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJR7yLxZIY4P"
   },
   "source": [
    "**This code creates a string `sum_expression` by joining the elements of the list `good_features` with the '+' operator. Each element in the list represents a feature, and the resulting string is the sum of these features.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzRoAk6lsedR"
   },
   "outputs": [],
   "source": [
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "\n",
    "sum_expression = \" + \".join(good_features)\n",
    "\n",
    "sum_expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z466ZgLIIbfh"
   },
   "source": [
    "**This code fills null values in the columns specified in the `good_features` list with 0, computes the score by summing up the values of these columns, and selects the 'score' and 'is_match' columns. Finally, it displays the resulting DataFrame `scored`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DU2aqwqYsgOx"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "scored = parsed.fillna(0, subset=good_features).withColumn('score', expr(sum_expression)).select('score', 'is_match')\n",
    "\n",
    "scored.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lurb6Mk1IeMl"
   },
   "source": [
    "**This code defines a function `crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame` to generate a cross-tabulation DataFrame based on a given threshold `t`. It selects records where the score is greater than or equal to the threshold (`score >= t`) and groups them by the binary classification ('above' or 'below' the threshold). Then, it pivots the 'is_match' column to create columns for 'true' and 'false' values and counts the occurrences of each combination. Finally, it returns the resulting cross-tabulation DataFrame.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opUYalwjsnaa"
   },
   "outputs": [],
   "source": [
    "def crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame:\n",
    "  return scored.selectExpr(f\"score >= {t} as above\", \"is_match\").groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj8bIqyPIiJd"
   },
   "source": [
    "**This code generates a cross-tabulation DataFrame by calling the `crossTabs()` function with the DataFrame `scored` and a threshold value of 4.0. It then displays the resulting DataFrame.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itHFbjmisrnX"
   },
   "outputs": [],
   "source": [
    "crossTabs(scored, 4.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbQYvDuiIlMh"
   },
   "source": [
    "**This code generates a cross-tabulation DataFrame by calling the `crossTabs()` function with the DataFrame `scored` and a threshold value of 2.0. It then displays the resulting DataFrame.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9cz7oO1str3"
   },
   "outputs": [],
   "source": [
    "crossTabs(scored, 2.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate precision, recall, and F1-score from true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP=cm1.filter(\"above==true\").select(\"true\").collect()[0].true\n",
    "TN=cm1.filter(\"above==true\").select(\"false\").collect()[0].false\n",
    "FP=cm1.filter(\"above==false\").select(\"true\").collect()[0].true\n",
    "FN=cm1.filter(\"above==false\").select(\"false\").collect()[0].false\n",
    "\n",
    "precision = TP/(TP + FP)\n",
    "recall = TP/(TP + FN)\n",
    "f1score = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(f\"Precision->{precision}\\nRecall->{recall}\\nF1-Score->{f1score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision->0.99713330148 <br>\n",
    "Recall->0.00363056914868 <br>\n",
    "F1-Score->0.007234796354 <br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPn9XrDjt7PiFevtNQ+qDPj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
