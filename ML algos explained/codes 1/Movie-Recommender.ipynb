{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUo0gqcpReIsJDPSfc93fG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Initializing Spark Context\n","\n","This code cell initializes a Spark Context for running Spark jobs.\n","\n","- **Setting Python Executables**: `os.environ['PYSPARK_PYTHON']` and `os.environ['PYSPARK_DRIVER_PYTHON']` set the Python executable paths for Spark.\n","- **Creating Spark Context**: `ps.SparkContext('local[*]')` creates a Spark Context using all available CPU cores on the local machine.\n","- **Spark Context Creation Message**: `\"Just created a SparkContext\"` is printed to indicate the successful creation of the Spark Context.\n","\n","Initializing the Spark Context is the first step in setting up a Spark environment for distributed data processing and analysis.\n"],"metadata":{"id":"amO_-Uqj5nJM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ser5uPhsxcBR"},"outputs":[],"source":["#import findspark\n","#findspark.init()\n","import os\n","import sys\n","import pyspark as ps\n","import warnings\n","from pyspark.sql import SQLContext\n","os.environ['PYSPARK_PYTHON'] = sys.executable\n","os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n","try:\n","  # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n","  sc = ps.SparkContext('local[*]')\n","  #sqlContext = SQLContext(sc)\n","  print(\"Just created a SparkContext\")\n","except ValueError:\n","  warnings.warn(\"SparkContext already exists in this scope\")"]},{"cell_type":"markdown","source":["## Running Unit Tests with PySpark\n","\n","This code cell defines and runs unit tests using the `unittest` framework with PySpark.\n","\n","- **Defining Test Case**: `TestRdd` class is defined inheriting from `unittest.TestCase`, containing a test method `test_take`.\n","- **Test Method**: `test_take` tests the `take` method on an RDD created from a parallelized collection.\n","- **Running Tests**: `run_tests()` function loads the test suite from the `TestRdd` class and runs it using `unittest.TextTestRunner`.\n","\n","Unit testing with PySpark allows for validating the behavior of Spark operations and transformations, ensuring correctness and reliability of Spark code.\n"],"metadata":{"id":"mFHNh8ku7_g5"}},{"cell_type":"code","source":["import unittest\n","import sys\n","\n","class TestRdd(unittest.TestCase):\n","  def test_take(self):\n","    input = sc.parallelize([1,2,3,4])\n","    self.assertEqual([1,2,3,4], input.take(4))\n","\n","\n","def run_tests():\n","  suite = unittest.TestLoader().loadTestsFromTestCase( TestRdd )\n","  unittest.TextTestRunner(verbosity=1,stream=sys.stderr).run( suite )\n","\n","run_tests()"],"metadata":{"id":"KdOWyDsFxjzR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","The help() function in Python provides documentation and information about the specified object or module. However, the help() function may not display the documentation directly in this interface.\n","\n","If you're looking for help on the sc object, which typically refers to the Spark Context in PySpark, you can access its documentation directly within your Python environment by executing help(sc) in a Python shell or Jupyter Notebook where PySpark is imported and initialized. This will display the documentation and available methods for the Spark Context object."],"metadata":{"id":"LY_BBHHj8Dxz"}},{"cell_type":"code","source":["help(sc)"],"metadata":{"id":"ZnEFoJF7xppA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Validation and Processing with PySpark\n","\n","This code cell defines a function `validate()` to check the presence of specific fields in JSON lines and processes JSON data using PySpark.\n","\n","- **Fields Definition**: `fields`, `fields2`, `fields3`, and `fields4` list the expected fields in the JSON data.\n","- **Validation Function**: `validate()` checks if all required fields are present in a JSON line. It returns `True` if all fields from `fields2` are present, otherwise `False`.\n","- **Reading JSON Data**: JSON data is read from the file 'data/movies.json' using `sc.textFile()` and stored in `reviews_raw`.\n","- **Data Processing**: The JSON data is parsed using `json.loads()` and filtered using the `validate()` function to ensure that only valid records are retained.\n","- **Caching**: The resulting RDD `reviews` is cached for faster access in subsequent operations.\n","\n","This code demonstrates data validation and processing using PySpark, ensuring that only records with the required fields are retained for further analysis.\n"],"metadata":{"id":"Ijgm1dZD8Lnm"}},{"cell_type":"code","source":["import json\n","\n","fields = ['product_id','user_id','score','time']\n","\n","fields2 = ['product_id','user_id','review','profile_name','helpfulness','score','time']\n","fields3 = ['product_id','user_id','time']\n","fields4 = ['user_id','score','time']\n","\n","def validate(line):\n","  for field in fields2:\n","    if field not in line: return False\n","  return True\n","\n","reviews_raw = sc.textFile('data/movies.json')\n","\n","reviews = reviews_raw.map(lambda line: json.loads(line)).filter(validate)\n","\n","reviews.cache()"],"metadata":{"id":"hD9jAonEx0xt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews.take(1)"],"metadata":{"id":"u5VrYAE0x8fM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analyzing Review Data\n","\n","This code cell calculates various statistics on the review data using PySpark operations.\n","\n","- **Number of Reviews**: `num_entries = reviews.count()` counts the total number of reviews in the dataset.\n","- **Number of Unique Movies**: `num_movies = reviews.groupBy(lambda entry: entry['product_id']).count()` groups the reviews by product ID and counts the number of unique movies.\n","- **Number of Unique Users**: `num_users = reviews.groupBy(lambda entry: entry['user_id']).count()` groups the reviews by user ID and counts the number of unique users.\n","- **Printing Statistics**: The statistics are printed, showing the total number of reviews, unique movies, and unique users.\n","\n","This code provides basic insights into the size and diversity of the review dataset, including the number of reviews, unique movies, and unique users.\n"],"metadata":{"id":"L4pnbOfF8O4I"}},{"cell_type":"code","source":["num_movies = reviews.groupBy(lambda entry: entry['product_id']).count()\n","num_users = reviews.groupBy(lambda entry: entry['user_id']).count()\n","num_entries = reviews.count()\n","\n","print (str(num_entries) + \" reviews of \" + str(num_movies) + \" movies by \" + str(num_users) + \" different people.\")"],"metadata":{"id":"yFeLkTWRyCgA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generating Movie Suggestions\n","\n","This code cell generates movie suggestions based on user reviews using PySpark operations.\n","\n","- **Map Phase**: `r1 = reviews.map(lambda r: ((r['product_id'],), 1))` maps each review to a tuple containing the product ID as a key and 1 as the value.\n","- **Reduce Phase**: `avg3 = r1.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))` reduces the tuples by key, summing up the values.\n","- **Filtering**: `avg3 = avg3.filter(lambda x: x[1][1] > 20 )` filters out movies with less than 20 reviews.\n","- **Sorting**: `avg3 = avg3.map(lambda x: ((x[1][0]+x[1][1],), x[0])).sortByKey(ascending=False)` calculates the average rating for each movie and sorts them in descending order of average rating.\n","\n","This code aims to provide movie suggestions based on user reviews, considering movies with a significant number of reviews and sorting them by average rating.\n"],"metadata":{"id":"6jbFKLNz8ZjF"}},{"cell_type":"code","source":["#Suggestion_users = reviews.filter(lambda entry: entry['user_id'])\n","\n","#for review in Suggestion_users.collect():\n","r1 = reviews.map(lambda r: ((r['product_id'],), 1))\n","avg3 = r1.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n","\n","avg3 = avg3.filter(lambda x: x[1][1] > 20 )\n","avg3 = avg3.map(lambda x: ((x[1][0]+x[1][1],), x[0])).sortByKey(ascending=False)"],"metadata":{"id":"uUjBzWKgyEw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Top 10 Movie Suggestions\n","\n","This code iterates through the top 10 movie suggestions generated based on user reviews and prints the Amazon URL for each movie along with the number of people who watched it.\n","\n","- **Iteration**: `for movie in avg3.take(10):` iterates through the top 10 movies in the `avg3` RDD.\n","- **Printing**: `print (\"http://www.amazon.com/dp/\" + movie[1][0] + \" WATCHED BY : \" + str(movie[0][0]) + \" PEOPLE\")` prints the Amazon URL for the movie (`movie[1][0]`) along with the number of people who watched it (`movie[0][0]`).\n","\n","This code provides direct links to the top 10 suggested movies on Amazon along with the number of people who have watched them.\n"],"metadata":{"id":"UK7syVXv8hxT"}},{"cell_type":"code","source":["for movie in avg3.take(10):\n","  print (\"http://www.amazon.com/dp/\" + movie[1][0] + \" WATCHED BY : \" + str(movie[0][0]) + \" PEOPLE\")"],"metadata":{"id":"qNFgS4dxyF-C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generating User Suggestions\n","\n","This code cell generates user suggestions based on their review activity using PySpark operations.\n","\n","- **Map Phase**: `r2 = reviews.map(lambda ru: ((ru['user_id'],), 1))` maps each review to a tuple containing the user ID as a key and 1 as the value.\n","- **Reduce Phase**: `avg2 = r2.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))` reduces the tuples by key, summing up the values.\n","- **Filtering**: `avg2 = avg2.filter(lambda x: x[1][1] > 20 )` filters out users with less than 20 reviews.\n","- **Sorting**: `avg2 = avg2.map(lambda x: ((x[1][0]+x[1][1],), x[0])).sortByKey(ascending=False )` calculates the total number of reviews for each user and sorts them in descending order.\n","\n","This code aims to provide user suggestions based on their review activity, considering users with a significant number of reviews and sorting them by total review count.\n"],"metadata":{"id":"vN_F7aY28l5M"}},{"cell_type":"code","source":["r2 = reviews.map(lambda ru: ((ru['user_id'],), 1))\n","avg2 = r2.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n","\n","avg2 = avg2.filter(lambda x: x[1][1] > 20 )\n","avg2 = avg2.map(lambda x: ((x[1][0]+x[1][1],), x[0])).sortByKey(ascending=False )"],"metadata":{"id":"br3_zidxyIx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Top 10 User Suggestions\n","\n","This code iterates through the top 10 user suggestions generated based on their review activity and prints the Amazon URL for each user along with the number of movies they have watched.\n","\n","- **Iteration**: `for user in avg2.take(10):` iterates through the top 10 users in the `avg2` RDD.\n","- **Printing**: `print (\"http://www.amazon.com/dp/\" + user[1][0] + \" WATCHED : \" + str(user[0][0]) + \" MOVIES\")` prints the Amazon URL for the user (`user[1][0]`) along with the number of movies they have watched (`user[0][0]`).\n","\n","This code provides direct links to the top 10 suggested users on Amazon along with the number of movies they have watched.\n"],"metadata":{"id":"qjOyhynK8qSm"}},{"cell_type":"code","source":["for movie in avg2.take(10):\n","  print (\"http://www.amazon.com/dp/\" + movie[1][0] + \" WATCHED : \" + str(movie[0][0]) + \" MOVIES\")"],"metadata":{"id":"CJ47MgRryKyJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Review Search for \"George\"\n","\n","This code cell searches for reviews written by users with \"George\" in their profile name and displays relevant information.\n","\n","- **Filtering**: `filtered = reviews.filter(lambda entry: \"George\" in entry['profile_name'])` filters reviews based on whether \"George\" is present in the profile name.\n","- **Counting Entries**: `print (\"Found \" + str(filtered.count()) + \" entries.\\n\")` prints the total number of entries found after filtering.\n","- **Iterating and Printing**: The code iterates through the filtered reviews and prints the rating, helpfulness, Amazon URL, summary, and full review text for each matching review.\n","\n","This code allows for searching and retrieving reviews written by users with \"George\" in their profile name, providing details such as rating, helpfulness, and review text.\n"],"metadata":{"id":"oLr-xgEO8vlf"}},{"cell_type":"code","source":["# Has someone written a review?\n","filtered = reviews.filter(lambda entry: \"George\" in entry['profile_name'])\n","print (\"Found \" + str(filtered.count()) + \" entries.\\n\")\n","\n","for review in filtered.collect():\n","  print (\"Rating: \" + str(review['score']) + \" and helpfulness: \" + review['helpfulness'])\n","  print (\"http://www.amazon.com/dp/\" + review['product_id'])\n","  print (review['summary'])\n","  print (review['review'])\n","  print (\"\\n\")"],"metadata":{"id":"GEUyChoxyL8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Best and Worst Rated Movies\n","\n","This code cell calculates the average rating for each movie and identifies the best and worst rated movies.\n","\n","- **Map Phase**: `reviews_by_movie = reviews.map(lambda r: ((r['product_id'],), r['score']))` maps each review to a tuple containing the product ID as a key and the score as the value.\n","- **Reduce Phase**: `avg = reviews_by_movie.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))` reduces the tuples by key, summing up the scores and counting the number of reviews.\n","- **Filtering**: `avg = avg.filter(lambda x: x[1][1] > 20 )` filters out movies with less than 20 reviews.\n","- **Sorting**: `avg = avg.map(lambda x: ((x[1][0]/x[1][1],), x[0])).sortByKey(ascending=True)` calculates the average rating for each movie and sorts them in ascending order of average rating.\n","\n","This code provides insights into the best and worst rated movies based on average user ratings, considering movies with a significant number of reviews.\n"],"metadata":{"id":"YlYQMuNT803g"}},{"cell_type":"code","source":["# Get best and worst rated movies\n","reviews_by_movie = reviews.map(lambda r: ((r['product_id'],), r['score']))\n","avg = reviews_by_movie.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n","\n","avg = avg.filter(lambda x: x[1][1] > 20 )\n","avg = avg.map(lambda x: ((x[1][0]/x[1][1],), x[0])).sortByKey(ascending=True)"],"metadata":{"id":"4G-wDNMmyWAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Top 10 Best and Worst Rated Movies\n","\n","This code iterates through the top 10 best and worst rated movies and prints their Amazon URL along with their average rating.\n","\n","- **Iteration**: `for movie in avg.take(10):` iterates through the top 10 movies in the `avg` RDD.\n","- **Printing**: `print (\"http://www.amazon.com/dp/\" + movie[1][0] + \" Rating: \" + str(movie[0][0]))` prints the Amazon URL for the movie (`movie[1][0]`) along with its average rating (`movie[0][0]`).\n","\n","This code provides direct links to the top 10 best and worst rated movies on Amazon along with their average ratings.\n"],"metadata":{"id":"U0EAhWHA86AH"}},{"cell_type":"code","source":["for movie in avg.take(10):\n","  print (\"http://www.amazon.com/dp/\" + movie[1][0] + \" Rating: \" + str(movie[0][0]))"],"metadata":{"id":"GRCNChFCyXla"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.1 Spark and Pandas"],"metadata":{"id":"fXCzxH7VyZFR"}},{"cell_type":"markdown","source":["## Creating Time Series RDD\n","\n","This code cell creates a time series RDD from the reviews RDD by mapping each entry to a dictionary containing the score and time attributes.\n","\n","- **Mapping**: `timeseries_rdd = reviews.map(lambda entry: {'score': entry['score'],'time': datetime.fromtimestamp(entry['time'])})` maps each entry in the reviews RDD to a dictionary with 'score' and 'time' attributes. The 'time' attribute is converted from a Unix timestamp to a datetime object using `datetime.fromtimestamp()`.\n","\n","This code aims to create a time series RDD for further analysis and visualization of review scores over time.\n"],"metadata":{"id":"0iajQByB9Ai3"}},{"cell_type":"code","source":["from datetime import datetime\n","\n","timeseries_rdd = reviews.map(lambda entry: {'score': entry['score'],'time': datetime.fromtimestamp(entry['time'])})"],"metadata":{"id":"f1DskFrnyai1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Time Series Analysis and Visualization\n","\n","This code cell performs time series analysis and visualization using Pandas and Matplotlib on the time series RDD created earlier.\n","\n","- **Pandas DataFrame**: `timeseries = pd.DataFrame(sample.collect(),columns=['score', 'time'])` converts the sampled RDD into a Pandas DataFrame with 'score' and 'time' columns.\n","- **Data Types**: `timeseries.score.astype('float64')` ensures that the 'score' column is of type float64.\n","- **Indexing**: `timeseries.set_index('time', inplace=True)` sets the 'time' column as the index of the DataFrame.\n","- **Resampling**: `Rsample = timeseries.score.resample('Y').count()` resamples the data annually ('Y'), counting the number of scores for each year.\n","- **Plotting**: `Rsample.plot()` plots the annual resampled data using Matplotlib.\n","- **Further Resampling and Plotting**: Similar resampling and plotting are performed for monthly ('M') and quarterly ('Q') intervals.\n","\n","This code provides insights into the distribution of review scores over time through visualizations at different temporal resolutions.\n"],"metadata":{"id":"_0CboYDN9Hk3"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","def parser(x):\n","  return datetime.strptime('190'+x, '%Y-%m')\n","\n","sample = timeseries_rdd.sample(withReplacement=False, fraction=20000.0/num_entries, seed=1134)\n","timeseries = pd.DataFrame(sample.collect(),columns=['score', 'time'])\n","print(timeseries.head(3))\n","timeseries.score.astype('float64')\n","#timeseries.time.astype('datetime64')\n","\n","timeseries.set_index('time', inplace=True)\n","Rsample = timeseries.score.resample('Y').count()\n","Rsample.plot()\n","Rsample2 = timeseries.score.resample('M').count()\n","Rsample2.plot()\n","Rsample3 = timeseries.score.resample('Q').count()\n","Rsample3.plot()"],"metadata":{"id":"8MjaSzMSybwu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.2 Matrix factorization"],"metadata":{"id":"qgDHVjnMygZC"}},{"cell_type":"markdown","source":["## Histogram of Average Ratings for Top Movies\n","\n","This code cell generates histograms of the average ratings for the top movies in the RDD.\n","\n","- **Iteration**: `for movie in avg.take(4):` iterates through the top 4 movies in the `avg` RDD.\n","- **Plotting**: `plt.bar(movie[1][0],movie[0][0])` creates a bar chart where the x-axis represents the movie and the y-axis represents the average rating.\n","- **Title, Labels**: `plt.title('Histogram of \\'AVERAGE RATING OF MOVIE\\'')`, `plt.xlabel('MOVIE')`, and `plt.ylabel('AVGRATING')` set the title and labels for the histogram.\n","\n","This code visualizes the average ratings of the top movies using histograms, allowing for easy comparison of their ratings.\n"],"metadata":{"id":"_bIerTv99M2L"}},{"cell_type":"code","source":["for movie in avg.take(4):\n","  plt.bar(movie[1][0],movie[0][0])\n","  plt.title('Histogram of \\'AVERAGE RATING OF MOVIE\\'')\n","  plt.xlabel('MOVIE')\n","  plt.ylabel('AVGRATING')"],"metadata":{"id":"GFGgbW5Zyha8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Histogram of Number of Movies Reviewed by Top Users\n","\n","This code cell generates histograms of the number of movies reviewed by the top users in the RDD.\n","\n","- **Iteration**: `for movie in avg2.take(3):` iterates through the top 3 users in the `avg2` RDD.\n","- **Plotting**: `plt.bar(movie[1][0],movie[0][0])` creates a bar chart where the x-axis represents the user and the y-axis represents the number of movies reviewed.\n","- **Title, Labels**: `plt.title('Histogram of \\'NUMBER OF MOVIES REVIEWED BY USER\\'')`, `plt.xlabel('USER')`, and `plt.ylabel('MOVIE COUNT')` set the title and labels for the histogram.\n","\n","This code visualizes the number of movies reviewed by the top users using histograms, allowing for easy comparison of their reviewing activity.\n"],"metadata":{"id":"NMDeDPBP9Qz5"}},{"cell_type":"code","source":["for movie in avg2.take(3):\n","  plt.bar(movie[1][0],movie[0][0])\n","  plt.title('Histogram of \\'NUMBER OF MOVIES REVIEWED BY USER\\'')\n","  plt.xlabel('USER')\n","  plt.ylabel('MOVIE COUNT')"],"metadata":{"id":"x4DYs6Fbykjy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Histogram of Movies Reviewed by Number of Users\n","\n","This code cell generates histograms of the number of users who reviewed each movie in the RDD.\n","\n","- **Iteration**: `for movie in avg3.take(4):` iterates through the top 4 movies in the `avg3` RDD.\n","- **Plotting**: `plt.bar(movie[1][0],movie[0][0])` creates a bar chart where the x-axis represents the movie and the y-axis represents the number of users who reviewed it.\n","- **Title, Labels**: `plt.title('Histogram of \\'MOVIES REVIEWED BY NUMBER OF USERS\\'')`, `plt.xlabel('MOVIE')`, and `plt.ylabel('USER COUNT')` set the title and labels for the histogram.\n","\n","This code visualizes the number of users who reviewed each movie using histograms, providing insights into the popularity of movies among users.\n"],"metadata":{"id":"KWNyE0Uh9UjL"}},{"cell_type":"code","source":["for movie in avg3.take(4):\n","  plt.bar(movie[1][0],movie[0][0])\n","  plt.title('Histogram of \\'MOVIES REVIEWED BY NUMBER OF USERS\\'')\n","  plt.xlabel('MOVIE')\n","  plt.ylabel('USER COUNT')"],"metadata":{"id":"PDecubVkyn4Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Collaborative Filtering with Alternating Least Squares (ALS)\n","\n","This code cell implements collaborative filtering using Alternating Least Squares (ALS) for recommendation.\n","\n","- **Import**: `from pyspark.mllib.recommendation import ALS` imports the ALS module.\n","- **Hashing**: `get_hash` function hashes the user and product IDs to integers for modeling.\n","- **Ratings**: `ratings = reviews.map(lambda entry: tuple([ get_hash(entry['user_id'].encode('utf-8')),get_hash(entry['product_id'].encode('utf-8')),int(entry['score']) ]))` transforms each entry in the reviews RDD into a tuple of (user ID, product ID, rating).\n","- **Train-Test Split**: `train_data` and `test_data` are generated by filtering ratings based on a hash-based train-test split.\n","- **Caching**: `train_data.cache()` caches the train data for efficient processing.\n","- **Print Stats**: The number of train and test samples are printed for inspection.\n","\n","This code sets up the data for collaborative filtering with ALS, allowing for the training and evaluation of recommendation models.\n"],"metadata":{"id":"Ot0D7CrD9YmY"}},{"cell_type":"code","source":["from pyspark.mllib.recommendation import ALS\n","from numpy import array\n","import hashlib\n","import math\n","\n","def get_hash(s):\n","  return int(hashlib.sha1(s).hexdigest(), 16) % (10 ** 8)\n","\n","\n","#Input format: [user, product, rating]\n","ratings = reviews.map(lambda entry: tuple([ get_hash(entry['user_id'].encode('utf-8')),get_hash(entry['product_id'].encode('utf-8')),int(entry['score']) ]))\n","\n","train_data = ratings.filter(lambda entry: ((entry[0]+entry[1]) % 10) >=2 )\n","test_data = ratings.filter(lambda entry: ((entry[0]+entry[1]) % 10) < 2 )\n","train_data.cache()\n","#train_data.union(train_data)\n","\n","print (\"Number of train samples: \" + str(train_data.count()))\n","print (\"Number of test samples: \" + str(test_data.count()))"],"metadata":{"id":"CP3ehLL6yrPb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation of Collaborative Filtering Model\n","\n","This code cell builds a recommendation model using Alternating Least Squares (ALS) and evaluates it on test data.\n","\n","- **Model Building**: `model = ALS.train(train_data, rank, numIterations)` trains the ALS model using the train data with the specified rank and number of iterations.\n","- **Prediction**: `predictions = model.predictAll(unknown)` generates predictions for unknown user-product pairs.\n","- **Evaluation**: The Mean Squared Error (MSE) is calculated to assess the model's performance on the test data.\n","\n","This code effectively builds and evaluates a recommendation model using collaborative filtering with ALS, providing insights into its predictive accuracy.\n","\n","## Building and Evaluating the Recommendation Model\n","\n","This code cell builds a recommendation model using Alternating Least Squares (ALS) and evaluates its performance.\n","\n","- **Model Parameters**: `rank = 20` and `numIterations = 20` define the rank of the latent factors and the number of iterations for ALS training.\n","- **Training**: `model = ALS.train(train_data, rank, numIterations)` trains the ALS model on the train data.\n","- **Evaluation**:\n","  - **Prediction**: `unknown = test_data.map(lambda entry: (int(entry[0]), int(entry[1])))` prepares the test data for prediction.\n","  - **Predictions**: `predictions = model.predictAll(unknown).map(lambda r: ((int(r[0]), int(r[1])), r[2]))` generates predictions for the test data.\n","  - **True and Predicted Values**: `true_and_predictions = test_data.map(lambda r: ((int(r[0]), int(r[1])), r[2])).join(predictions)` joins true ratings with predictions.\n","  - **Mean Squared Error (MSE)**: `MSE = true_and_predictions.map(lambda r: (int(r[1][0]) - int(r[1][1])**2).reduce(lambda x, y: x + y)/true_and_predictions.count())` calculates the MSE between true and predicted ratings.\n","\n","This code trains the recommendation model using ALS and evaluates its performance using MSE.\n","\n"],"metadata":{"id":"SGFQ4frV9c2F"}},{"cell_type":"code","source":["# Build the recommendation model using Alternating Least Squares\n","from math import sqrt\n","rank = 20\n","numIterations = 20\n","model = ALS.train(train_data, rank, numIterations)\n","\n","def convertToFloat(lines):\n","  returnedLine = []\n","  for x in lines:\n","    returnedLine.append(float(x))\n","  return returnedLine\n","\n","# Evaluate the model on test data\n","unknown = test_data.map(lambda entry: (int(entry[0]), int(entry[1])))\n","predictions = model.predictAll(unknown).map(lambda r: ((int(r[0]), int(r[1])), r[2]))\n","true_and_predictions = test_data.map(lambda r: ((int(r[0]), int(r[1])), r[2])).join(predictions)\n","MSE = true_and_predictions.map(lambda r: (int(r[1][0]) - int(r[1][1])**2).reduce(lambda x, y: x + y)/true_and_predictions.count())"],"metadata":{"id":"BKEBu_GXyz7v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This output shows the first 10 elements of the true_and_predictions RDD, where each element consists of a tuple containing the user ID, product ID, true rating, and predicted rating"],"metadata":{"id":"vthUFHHw9tkv"}},{"cell_type":"code","source":["true_and_predictions.take(10)"],"metadata":{"id":"-G6FQpd7y4mq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.3 No demo without a word count example!"],"metadata":{"id":"zZS25LK3y6CU"}},{"cell_type":"markdown","source":["## Filtering Reviews for Word Analysis\n","\n","This code cell filters reviews based on their ratings to analyze words used in positive (5.0) and negative (1.0) reviews.\n","\n","- **Parameters**:\n","  - `min_occurrences = 10`: Minimum occurrences required for a word to be considered.\n","\n","- **Positive Reviews**:\n","  - `good_reviews`: Filters reviews with a score of 5.0.\n","  - `good_words`: Splits each review into words and counts their occurrences. Words occurring less than `min_occurrences` times are filtered out.\n","\n","- **Negative Reviews**:\n","  - `bad_reviews`: Filters reviews with a score of 1.0.\n","  - `bad_words`: Splits each review into words and counts their occurrences. Words occurring less than `min_occurrences` times are filtered out.\n"],"metadata":{"id":"jaeDaoW590iK"}},{"cell_type":"code","source":["min_occurrences = 10\n","\n","good_reviews = reviews.filter(lambda line: line['score']==5.0)\n","bad_reviews = reviews.filter(lambda line: line['score']==1.0)\n","\n","good_words = good_reviews.flatMap(lambda line: line['review'].split(' '))\n","num_good_words = good_words.count()\n","\n","good_words = good_words.map(lambda word: (word.strip(), 1)).reduceByKey(lambda a, b: a+b).filter(lambda word_count: word_count[1] > min_occurrences)\n","\n","bad_words = bad_reviews.flatMap(lambda line: line['review'].split(' '))\n","num_bad_words = bad_words.count()\n","\n","bad_words = bad_words.map(lambda word: (word.strip(), 1)).reduceByKey(lambda a, b: a+b).filter(lambda word_count: word_count[1] > min_occurrences)"],"metadata":{"id":"7pvLgGDqy7aM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculating Word Frequencies\n","\n","This code cell calculates the frequencies of words found in positive and negative reviews.\n","\n","- **Positive Reviews**:\n","  - `frequency_good`: Calculates the frequency of each word in positive reviews by dividing its count by the total number of words in positive reviews.\n","\n","- **Negative Reviews**:\n","  - `frequency_bad`: Calculates the frequency of each word in negative reviews by dividing its count by the total number of words in negative reviews.\n"],"metadata":{"id":"083u6gV095R4"}},{"cell_type":"code","source":["# Calculate the word frequencies\n","frequency_good = good_words.map(lambda word: ((word[0],), float(word[1])/num_good_words))\n","frequency_bad = bad_words.map(lambda word: ((word[0],), float(word[1])/num_bad_words))"],"metadata":{"id":"4mbGSR69y_04"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Joining Word Frequencies\n","\n","This code cell joins the word frequencies calculated for positive and negative reviews.\n","\n","- **Input**:\n","  - `frequency_good`: Frequencies of words in positive reviews.\n","  - `frequency_bad`: Frequencies of words in negative reviews.\n","\n","- **Output**:\n","  - `joined_frequencies`: Joined frequencies of words in both positive and negative reviews.\n"],"metadata":{"id":"2oNzgRUp982n"}},{"cell_type":"code","source":["# Join the word frequencies of the good and bad reviews\n","joined_frequencies = frequency_good.join(frequency_bad)"],"metadata":{"id":"B1GpFMy2zBd0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculating Relative Difference of Word Frequencies\n","\n","This code cell calculates the relative difference of word frequencies between positive and negative reviews. It sorts the dataset to identify the most significant expressions for characterizing either positively or negatively rated movies.\n"],"metadata":{"id":"nrGIQllM-AyU"}},{"cell_type":"code","source":["# Calculate the relative difference of each word frequency in the good and bad reviews.\n","# Sort the dataset to get the most significant expressions for the characterization of either a positively\n","# or negatively rated movie.\n","\n","import math\n","\n","def relative_difference(a, b):\n","  return math.fabs(a-b)/a\n","\n","result = joined_frequencies.map(lambda f: ((relative_difference(f[1][0], f[1][1]),), f[0][0]) ).sortByKey(ascending=False)"],"metadata":{"id":"ZLq8O5BwzD37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result.take(50)"],"metadata":{"id":"K_uzBlJ1zGHx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Histogram of Sentiment Analysis\n","\n","This code cell creates a histogram of sentiment analysis based on the relative difference of word frequencies between positive and negative reviews. It displays the top 7 significant expressions for characterizing movie sentiment.\n","\n","- **X-axis**: Word\n","- **Y-axis**: Number of occurrences\n"],"metadata":{"id":"VMx_Rqt--HLn"}},{"cell_type":"code","source":["for movie in result.take(7):\n","  plt.bar(movie[1],movie[0][0])\n","  plt.title('Histogram of \\'SENTIMENT ANALYSIS\\'')\n","  plt.xlabel('WORD')\n","  plt.ylabel('NUMBER OF OCCURANCES')"],"metadata":{"id":"RR2kL0uazH10"},"execution_count":null,"outputs":[]}]}