{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOC5OzMR5eJSCEdif1W7aXv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Setting Up PySpark Environment\n","\n","This code cell sets up the PySpark environment.\n","\n","- **Input**: None\n","- **Actions**:\n","  - Sets the Python executable paths for PySpark.\n","  - Initializes the SparkContext.\n","- **Output**: SparkContext initialized for PySpark.\n"],"metadata":{"id":"LQd83CvxA71n"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vfj8lxm9vrhC"},"outputs":[],"source":["import pyspark\n","import os\n","import sys\n","from pyspark import SparkContext\n","os.environ['PYSPARK_PYTHON'] = sys.executable\n","os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n","\n","from pyspark.sql import SparkSession"]},{"cell_type":"markdown","source":["## Initializing Spark Session\n","\n","This code cell initializes a Spark session with specific configurations.\n","\n","- **Configuration**:\n","  - Spark driver memory: 16 GB\n","  - Application name: chapter_8\n","- **Output**: SparkSession initialized with the specified configurations.\n"],"metadata":{"id":"u3KNy-g4BI-K"}},{"cell_type":"code","source":["spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_8').getOrCreate()"],"metadata":{"id":"uOZzMVHYvwxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.1 Preparing the Data"],"metadata":{"id":"Ad90yE41vyIC"}},{"cell_type":"markdown","source":["## Reading Multiple CSV Files\n","\n","This code cell reads multiple CSV files into a Spark DataFrame.\n","\n","- **Files Read**: ABAX.csv, AAME.csv, AEPI.csv\n","- **Headers**: The first row of each CSV file is considered as the header.\n","- **Schema Inference**: Automatically inferring the schema of the DataFrame.\n","- **Output**: Displaying the first two rows of the DataFrame.\n"],"metadata":{"id":"QjftuIEwBMhn"}},{"cell_type":"code","source":["stocks = spark.read.csv([\"data/stocksA/ABAX.csv\",\"data/stocksA/AAME.csv\",\"data/stocksA/AEPI.csv\"], header='true', inferSchema='true')\n","\n","#stocks=spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load('C:/Users/HP/Desktop/aas-pyspark-edition/data/stocksA/AAIT.csv').load('C:/Users/HP/Desktop/aas-pyspark-edition/data/stocksA/AAME.csv')\n","\n","stocks.show(2)"],"metadata":{"id":"utVk0DgSvzqL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extracting Symbol from File Path\n","\n","This code cell extracts the symbol from the file path of each stock and adds it as a new column to the DataFrame.\n","\n","- **Method**: Extracts the symbol from the file path using string manipulation functions.\n","- **Output**: Displaying the first two rows of the DataFrame with the new \"Symbol\" column.\n"],"metadata":{"id":"HA3jDxM5BQBE"}},{"cell_type":"code","source":["from pyspark.sql import functions as fun\n","\n","stocks = stocks.withColumn(\"Symbol\", fun.input_file_name()).withColumn(\"Symbol\", fun.element_at(fun.split(\"Symbol\", \"/\"), -1)).withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))\n","\n","stocks.show(2)"],"metadata":{"id":"fnYbHiZ3v1qz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading and Extracting Symbol from File Path\n","\n","This code cell loads the factor data from multiple CSV files and extracts the symbol from the file path, adding it as a new column to the DataFrame.\n","\n","- **Input**: Path to multiple CSV files containing factor data.\n","- **Method**: Loads the CSV files into a DataFrame and extracts the symbol from the file path using string manipulation functions.\n","- **Output**: DataFrame with the symbol extracted from the file path and added as a new column.\n"],"metadata":{"id":"h5-NyB5qBU5d"}},{"cell_type":"code","source":["factors = spark.read.csv([\"data/stocksA/ABAX.csv\",\"data/stocksA/AAME.csv\",\"data/stocksA/AEPI.csv\"], header='true', inferSchema='true')\n","\n","factors = factors.withColumn(\"Symbol\", fun.input_file_name()).withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"/\"), -1)).withColumn(\"Symbol\",fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))"],"metadata":{"id":"FrjjPGtFv4py"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Filtering Stocks Data by Symbol Count\n","\n","This code cell filters the stocks data by the count of symbols, ensuring that only symbols with more than a specified count are retained.\n","\n","- **Input**: DataFrame containing stocks data with a column 'Symbol'.\n","- **Method**: Uses a window function to calculate the count of each symbol and filters the DataFrame to retain only symbols with a count greater than a specified threshold.\n","- **Output**: DataFrame with stocks data filtered by symbol count.\n"],"metadata":{"id":"D16c2mTyBaym"}},{"cell_type":"code","source":["from pyspark.sql import Window\n","\n","stocks = stocks.withColumn('count', fun.count('Symbol').over(Window.partitionBy('Symbol'))).filter(fun.col('count') > 260*5 + 10)"],"metadata":{"id":"55k30htUv6kM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setting Legacy Time Parser Policy\n","\n","This code cell sets the time parser policy of Spark SQL to legacy.\n","\n","- **Input**: None\n","- **Method**: Uses the `set` function of Spark SQL to set the `spark.sql.legacy.timeParserPolicy` property to `LEGACY`.\n","- **Output**: Spark SQL configuration with the legacy time parser policy set.\n"],"metadata":{"id":"q_jRd-QaBgFB"}},{"cell_type":"code","source":["spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"],"metadata":{"id":"U1KeNvnFv8KW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Converting Date Format\n","\n","This code cell converts the date format in the `Date` column to a standard date format.\n","\n","- **Input**: DataFrame `stocks` with a column named `Date` containing date values in the format 'dd-MMM-yy'.\n","- **Method**: Uses the `withColumn` function along with `to_timestamp` and `to_date` functions from the `functions` module of PySpark to convert the date format to a standard format.\n","- **Output**: DataFrame `stocks` with the `Date` column converted to a standard date format.\n"],"metadata":{"id":"ePLmnLO1Bjhw"}},{"cell_type":"code","source":["stocks = stocks.withColumn('Date',fun.to_date(fun.to_timestamp(fun.col('Date'),'dd-MMM-yy')))\n","\n","stocks.printSchema()"],"metadata":{"id":"Rr6wvF7Cv-Rt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Filtering Date Range\n","\n","This code cell filters the DataFrame `stocks` to include only rows with dates falling within a specific range.\n","\n","- **Input**: DataFrame `stocks` containing a column named `Date` with date values.\n","- **Method**: Uses the `filter` function to select rows where the `Date` column values are greater than or equal to October 23, 2009, and less than or equal to October 23, 2014.\n","- **Output**: DataFrame `stocks` containing only rows with dates falling within the specified range.\n"],"metadata":{"id":"Yn-UTGGmBnpF"}},{"cell_type":"code","source":["from datetime import datetime\n","\n","stocks = stocks.filter(fun.col('Date') >= datetime(2009, 10, 23)).filter(fun.col('Date') <= datetime(2014, 10, 23))"],"metadata":{"id":"5b7HHi5NwAAt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Filtering Date Range\n","\n","This code cell filters the DataFrame `factors` to include only rows with dates falling within a specific range.\n","\n","- **Input**: DataFrame `factors` containing a column named `Date` with date values.\n","- **Method**: Uses the `filter` function to select rows where the `Date` column values are greater than or equal to October 23, 2009, and less than or equal to October 23, 2014.\n","- **Output**: DataFrame `factors` containing only rows with dates falling within the specified range.\n"],"metadata":{"id":"HQkvia8cBtcg"}},{"cell_type":"code","source":["factors = factors.withColumn('Date',fun.to_date(fun.to_timestamp(fun.col('Date'),'dd-MMM-yy')))\n","\n","factors = factors.filter(fun.col('Date') >= datetime(2009, 10, 23)).filter(fun.col('Date') <= datetime(2014, 10, 23))"],"metadata":{"id":"ef8pF6WVwCHn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Converting Spark DataFrames to Pandas DataFrames\n","\n","This code cell converts the Spark DataFrames `stocks` and `factors` to Pandas DataFrames.\n","\n","- **Input**: Spark DataFrames `stocks` and `factors`.\n","- **Method**: Uses the `toPandas()` function to convert the Spark DataFrames to Pandas DataFrames.\n","- **Output**: Pandas DataFrames `stocks_pd_df` and `factors_pd_df` containing the data from the respective Spark DataFrames.\n"],"metadata":{"id":"tKe78OLYBxmk"}},{"cell_type":"code","source":["stocks_pd_df = stocks.toPandas()\n","factors_pd_df = factors.toPandas()\n","\n","factors_pd_df.head(5)"],"metadata":{"id":"GK7LvO-TwD14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.2 Determining the Factor Weights"],"metadata":{"id":"OWbu95u_wGHN"}},{"cell_type":"markdown","source":["## Calculating Rolling Returns\n","\n","This code calculates rolling returns for stocks and factors.\n","\n","- **Input**: Pandas DataFrames `stocks_pd_df` and `factors_pd_df`.\n","- **Parameters**: `n_steps` set to 10.\n","- **Method**:\n","  - Defines a custom function `my_fun(x)` to calculate returns based on the closing prices.\n","  - Groups the data by symbol and applies the rolling function to calculate returns over a window of `n_steps`.\n","- **Output**: DataFrames `stock_returns` and `factors_returns` containing the rolling returns for stocks and factors respectively.\n"],"metadata":{"id":"jNj-uXs3B2Sm"}},{"cell_type":"code","source":["n_steps = 10\n","\n","def my_fun(x):\n","  return ((x.iloc[-1] - x.iloc[0]) / x.iloc[0])\n","\n","stock_returns = stocks_pd_df.groupby('Symbol').Close.rolling(window=n_steps).apply(my_fun)\n","factors_returns = factors_pd_df.groupby('Symbol').Close.rolling(window=n_steps).apply(my_fun)\n","\n","stock_returns = stock_returns.reset_index().sort_values('level_1').reset_index()\n","factors_returns = factors_returns.reset_index().sort_values('level_1').reset_index()"],"metadata":{"id":"qFB4_SyHwHwj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Combining Stocks and Factors DataFrames\n","\n","This code combines the stocks and factors DataFrames, adding rolling returns to the stocks DataFrame and organizing the factors DataFrame.\n","\n","- **Input**: Pandas DataFrames `stocks_pd_df`, `stock_returns`, `factors_pd_df`, and `factors_returns`.\n","- **Output**: Combined DataFrames `stocks_pd_df_with_returns` and `factors_pd_df_with_returns`.\n","- **Method**:\n","  - Adds the rolling returns to the stocks DataFrame as a new column named `stock_returns`.\n","  - Adds the squared rolling returns to the factors DataFrame as a new column named `factors_returns_squared`.\n","  - Pivots the factors DataFrame to organize the data.\n","  - Resets the index of the factors DataFrame.\n"],"metadata":{"id":"WIK8lc1BB76z"}},{"cell_type":"code","source":["# Create combined stocks DF\n","stocks_pd_df_with_returns = stocks_pd_df.assign(stock_returns = stock_returns['Close'])\n","\n","# Create combined factors DF\n","factors_pd_df_with_returns = factors_pd_df.assign(factors_returns = factors_returns['Close'],factors_returns_squared = factors_returns['Close']**2)\n","\n","factors_pd_df_with_returns = factors_pd_df_with_returns.pivot(index='Date',columns='Symbol',values=['factors_returns', 'factors_returns_squared'])\n","\n","factors_pd_df_with_returns.columns = factors_pd_df_with_returns.columns.to_series().str.join('_').reset_index()[0]\n","\n","factors_pd_df_with_returns = factors_pd_df_with_returns.reset_index()\n","\n","print(factors_pd_df_with_returns.head(1))"],"metadata":{"id":"bFHVhp5cwNyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(factors_pd_df_with_returns.columns)"],"metadata":{"id":"Eq4YojTjwRUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Linear Regression Analysis\n","\n","This code performs linear regression analysis on the combined stocks and factors DataFrame.\n","\n","- **Input**: Combined Pandas DataFrame `stocks_factors_combined_df` containing stocks and factors data.\n","- **Output**: DataFrame `coefs_per_stock` containing coefficients of the linear regression model for each stock.\n","- **Method**:\n","  - Merges the stocks and factors DataFrames.\n","  - Drops NaN values from the DataFrame.\n","  - Performs linear regression analysis for each stock.\n","  - Stores the coefficients of the linear regression model in the `coefs_per_stock` DataFrame.\n"],"metadata":{"id":"TCoSFjWSCEhh"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","\n","# For each stock, create input DF for linear regression training\n","\n","stocks_factors_combined_df = pd.merge(stocks_pd_df_with_returns,factors_pd_df_with_returns,how=\"left\", on=\"Date\")\n","\n","feature_columns = list(stocks_factors_combined_df.columns[-6:])\n","\n","with pd.option_context('mode.use_inf_as_na', True):\n","  stocks_factors_combined_df = stocks_factors_combined_df.dropna(subset=feature_columns + ['stock_returns'])\n","\n","def find_ols_coef(df):\n","  y = df[['stock_returns']].values\n","  X = df[feature_columns]\n","\n","  regr = LinearRegression()\n","  regr_output = regr.fit(X, y)\n","\n","  return list(df[['Symbol']].values[0]) + list(regr_output.coef_[0])\n","\n","coefs_per_stock = stocks_factors_combined_df.groupby('Symbol').apply(find_ols_coef)\n","\n","coefs_per_stock = pd.DataFrame(coefs_per_stock).reset_index()\n","coefs_per_stock.columns = ['symbol', 'factor_coef_list']\n","\n","coefs_per_stock = pd.DataFrame(coefs_per_stock.factor_coef_list.tolist(),index=coefs_per_stock.index,columns = ['Symbol'] + feature_columns)\n","\n","coefs_per_stock"],"metadata":{"id":"v8nUvmtrwSyA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.3 Sampling"],"metadata":{"id":"5lIP2q9xwsZp"}},{"cell_type":"markdown","source":["## Kernel Density Estimation Plot\n","\n","This code generates a Kernel Density Estimation (KDE) plot for the returns of a specific factor.\n","\n","- **Input**: Pandas DataFrame `factors_returns` containing factor returns.\n","- **Output**: KDE plot of the returns for a specific factor.\n","- **Method**:\n","  - Selects the returns for a specific factor using the `loc` function.\n","  - Generates the KDE plot using the `plot.kde()` function.\n"],"metadata":{"id":"0VUm-W5oCKZJ"}},{"cell_type":"code","source":["samples = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[0]]['Close']\n","\n","samples.plot.kde()"],"metadata":{"id":"Mtppdvufwtob"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Correlation Analysis of Factor Returns\n","\n","This code performs correlation analysis on the factor returns for three different factors.\n","\n","- **Input**: Pandas DataFrame `factors_returns` containing factor returns.\n","- **Output**: Correlation matrix of factor returns for three different factors (`f1`, `f2`, `f3`).\n","- **Method**:\n","  - Selects the returns for each factor using the `loc` function.\n","  - Creates a DataFrame with three columns (`f1`, `f2`, `f3`).\n","  - Calculates the correlation matrix using the `corr()` function.\n"],"metadata":{"id":"9DsUqkb_CN2B"}},{"cell_type":"code","source":["f_1 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[0]]['Close']\n","f_2 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[1]]['Close']\n","f_3 = factors_returns.loc[factors_returns.Symbol == factors_returns.Symbol.unique()[2]]['Close']\n","\n","print(f_1.size,len(f_2),f_3.size)\n","pd.DataFrame({'f1': list(f_1)[1:1040], 'f2': list(f_2)[1:1040], 'f3': list(f_3)}).corr()"],"metadata":{"id":"sbRVKXG6wxK8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculation of Covariance and Mean of Factor Returns\n","\n","This code calculates the covariance matrix and mean of factor returns for three different factors.\n","\n","- **Input**: Pandas DataFrame `factors_returns` containing factor returns for three factors (`f1`, `f2`, `f3`).\n","- **Output**: Covariance matrix and mean vector of factor returns.\n","- **Method**:\n","  - Constructs a DataFrame with three columns (`f1`, `f2`, `f3`) containing factor returns.\n","  - Calculates the covariance matrix using the `cov()` function and converts it to a numpy array.\n","  - Calculates the mean vector using the `mean()` function.\n"],"metadata":{"id":"GPj7fnFJCRqK"}},{"cell_type":"code","source":["factors_returns_cov = pd.DataFrame({'f1': list(f_1)[1:1040],\n","                                    'f2': list(f_2)[1:1040],\n","                                    'f3': list(f_3)}).cov().to_numpy()\n","\n","factors_returns_mean = pd.DataFrame({'f1': list(f_1)[1:1040],\n","                                    'f2': list(f_2)[1:1040],\n","                                    'f3': list(f_3)}).mean()"],"metadata":{"id":"VAZKZRzRw0mL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code generates random samples from a multivariate normal distribution defined by the mean vector `factors_returns_mean` and the covariance matrix `factors_returns_cov`, using the `multivariate_normal` function from the numpy.random module.\n"],"metadata":{"id":"unJ44h-YCWsh"}},{"cell_type":"code","source":["from numpy.random import multivariate_normal\n","\n","multivariate_normal(factors_returns_mean, factors_returns_cov)"],"metadata":{"id":"8AOaXBOhw15M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.4 Running the Trials"],"metadata":{"id":"h-zK4v0xw337"}},{"cell_type":"markdown","source":["Broadcasting variables `coefs_per_stock`, `feature_columns`, `factors_returns_mean`, and `factors_returns_cov` using Spark's `broadcast` method for efficient sharing across Spark workers.\n"],"metadata":{"id":"vQL9IVSbCgtk"}},{"cell_type":"code","source":["b_coefs_per_stock = spark.sparkContext.broadcast(coefs_per_stock)\n","b_feature_columns = spark.sparkContext.broadcast(feature_columns)\n","b_factors_returns_mean = spark.sparkContext.broadcast(factors_returns_mean)\n","b_factors_returns_cov = spark.sparkContext.broadcast(factors_returns_cov)"],"metadata":{"id":"OUrhl39yw55L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code imports the `IntegerType` from the `pyspark.sql.types` module. It sets up parameters such as `parallelism`, `num_trials`, and `base_seed`. Then, it creates a list of seeds ranging from `base_seed` to `base_seed + parallelism`, creates a DataFrame from these seeds using Spark's `createDataFrame` method, and repartitions the DataFrame into `parallelism` partitions.\n"],"metadata":{"id":"Sal8WCPSCnQu"}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\n","\n","parallelism = 1000\n","num_trials = 1000000\n","base_seed = 1496\n","\n","seeds = [b for b in range(base_seed,base_seed + parallelism)]\n","seedsDF = spark.createDataFrame(seeds, IntegerType())\n","\n","seedsDF = seedsDF.repartition(parallelism)"],"metadata":{"id":"9WUvbcrgw8HJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code defines a Python function `calculate_trial_return(x)` which generates a list of trial returns based on the given seed `x`. Within the function, random factors are generated using the mean and covariance values broadcasted earlier. Then, the function computes returns per stock using coefficients and random factors. The function returns a list of trial returns.\n","\n","It also creates a user-defined function (`udf_return`) using Spark's `udf` function, which applies the `calculate_trial_return` function to each element in a Spark DataFrame column, outputting an array of trial returns.\n"],"metadata":{"id":"VECwbPkHDhON"}},{"cell_type":"code","source":["import random\n","from numpy.random import seed\n","from pyspark.sql.types import LongType, ArrayType, DoubleType\n","from pyspark.sql.functions import udf\n","\n","def calculate_trial_return(x):\n","  # return x\n","  trial_return_list = []\n","\n","  for i in range(int(num_trials/parallelism)):\n","    random_int = random.randint(0, num_trials*num_trials)\n","\n","    seed(x)\n","\n","    random_factors = multivariate_normal(b_factors_returns_mean.value,b_factors_returns_cov.value)\n","\n","    coefs_per_stock_df = b_coefs_per_stock.value\n","    returns_per_stock = (coefs_per_stock_df[b_feature_columns.value] * (list(random_factors) + list(random_factors**2)))\n","\n","    trial_return_list.append(float(returns_per_stock.sum(axis=1).sum()/b_coefs_per_stock.value.size))\n","\n","  return trial_return_list\n","\n","udf_return = udf(calculate_trial_return, ArrayType(DoubleType()))"],"metadata":{"id":"3RYew91qw-VZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code applies the `udf_return` user-defined function to each element in the 'value' column of the DataFrame `seedsDF`, creating a new column 'trial_return'. It then explodes the array elements in the 'trial_return' column into separate rows. Finally, it caches the resulting DataFrame `trials` into memory for faster access.\n"],"metadata":{"id":"rY9WOChrDoD8"}},{"cell_type":"code","source":["from pyspark.sql.functions import col, explode\n","\n","trials = seedsDF.withColumn(\"trial_return\", udf_return(col(\"value\")))\n","trials = trials.select('value', explode('trial_return').alias('trial_return'))\n","\n","trials.cache()"],"metadata":{"id":"IjzXX7g6xHnk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.5 TAKES SOME TIME"],"metadata":{"id":"odNsR8UExKvC"}},{"cell_type":"markdown","source":["This code calculates the approximate quantile(s) of the 'trial_return' column in the `trials` DataFrame. It specifically computes the 5th percentile (0.05 quantile) with a relative error of 0.0.\n"],"metadata":{"id":"gVrhG1kNDvtc"}},{"cell_type":"code","source":["trials.approxQuantile('trial_return', [0.05], 0.0)"],"metadata":{"id":"eFjC_0ACxL3E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code sorts the `trials` DataFrame by the 'trial_return' column in ascending order, then limits the result to a fraction (1/20th) of the total rows. It calculates the average of the 'trial_return' values within this limited subset and displays the result.\n"],"metadata":{"id":"WSkOwPpuDzKs"}},{"cell_type":"code","source":["trials.orderBy(col('trial_return').asc()).limit(int(trials.count()/20)).agg(fun.avg(col(\"trial_return\"))).show()"],"metadata":{"id":"jSIFGKXFxNaf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.6 Visualizing the Distribution of Returns"],"metadata":{"id":"Hd9quDwFxPZd"}},{"cell_type":"markdown","source":["This code converts the `trials` DataFrame to a Pandas DataFrame named `mytrials`. Then, it generates a line plot using the `plot.line()` method, visualizing the data in the Pandas DataFrame.\n"],"metadata":{"id":"g97-hhGZD3Vy"}},{"cell_type":"code","source":["import pandas\n","\n","mytrials=trials.toPandas()\n","mytrials.plot.line()"],"metadata":{"id":"oZ2nhX7VxR47"},"execution_count":null,"outputs":[]}]}