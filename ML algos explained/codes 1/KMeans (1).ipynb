{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMcfcJfAtNZ9vM6HeICtu8V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This code imports necessary modules and sets environment variables for PySpark to use the correct Python executable. It imports `SparkContext` from `pyspark` and `SparkSession` from `pyspark.sql`.\n"],"metadata":{"id":"jinp1eY5GLlp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VymElEEquWig"},"outputs":[],"source":["import pyspark\n","import os\n","import sys\n","from pyspark import SparkContext\n","os.environ['PYSPARK_PYTHON'] = sys.executable\n","os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n","\n","from pyspark.sql import SparkSession"]},{"cell_type":"markdown","source":["This code initializes a SparkSession named 'chapter_5' with a driver memory of 16 GB using the `SparkSession.builder.config()` method.\n"],"metadata":{"id":"F8PkYG9PGNMU"}},{"cell_type":"code","source":["spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_5').getOrCreate()"],"metadata":{"id":"Sw0q03RNueyA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.1 A First Take on Clustering"],"metadata":{"id":"8XYmUoUPujth"}},{"cell_type":"markdown","source":["This code reads a CSV file located at \"data/kddcup.data_10_percent_corrected\" into a Spark DataFrame `data_without_header`, treating the first row as data rather than a header. It then assigns column names to the DataFrame based on a predefined list `column_names`. Finally, it creates a new DataFrame `data` with the specified column names."],"metadata":{"id":"hkY2b0npGQY1"}},{"cell_type":"code","source":["data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"data/kddcup.data_10_percent_corrected\")\n","\n","column_names = [ \"duration\", \"protocol_type\", \"service\", \"flag\",\n","  \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n","  \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n","  \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n","  \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n","  \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n","  \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n","  \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n","  \"dst_host_count\", \"dst_host_srv_count\",\n","  \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n","  \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n","  \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n","  \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n","  \"label\"]\n","\n","data = data_without_header.toDF(*column_names)"],"metadata":{"id":"Jk-1no3SugYu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code selects the 'label' column from the DataFrame `data`, groups it by unique values in the 'label' column, counts the occurrences of each label, orders the result by the count in descending order, and displays the top 25 labels along with their counts.\n"],"metadata":{"id":"Vre3f-QrGVUX"}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(25)"],"metadata":{"id":"2ZNT2ladujON"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code imports necessary modules and classes from PySpark for performing K-means clustering. It first drops non-numeric columns from the DataFrame `data` and caches the resulting DataFrame `numeric_only`. Then, it sets up a `VectorAssembler` to assemble feature vectors from numeric columns, followed by initializing a K-means model. A pipeline is constructed to execute these transformations and modeling steps sequentially. After fitting the pipeline to the data, the cluster centers from the trained K-means model are printed using the `clusterCenters()` method."],"metadata":{"id":"TaFOXELLGYys"}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.clustering import KMeans, KMeansModel\n","from pyspark.ml import Pipeline\n","\n","numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n","\n","assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n","\n","kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n","\n","pipeline = Pipeline().setStages([assembler, kmeans])\n","pipeline_model = pipeline.fit(numeric_only)\n","kmeans_model = pipeline_model.stages[1]\n","\n","from pprint import pprint\n","pprint(kmeans_model.clusterCenters())"],"metadata":{"id":"IidDPMgbupRf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code applies the trained pipeline model `pipeline_model` to the `numeric_only` DataFrame, generating predictions and adding a 'cluster' column to the DataFrame `with_cluster`. It then selects the 'cluster' and 'label' columns, groups by both columns, counts the occurrences of each combination, orders the result first by cluster and then by count in descending order, and displays the top 25 combinations of clusters and labels along with their counts.\n"],"metadata":{"id":"hcDABEbOGcvi"}},{"cell_type":"code","source":["with_cluster = pipeline_model.transform(numeric_only)\n","\n","with_cluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)"],"metadata":{"id":"OW85BTJQurz_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.2 Choosing k"],"metadata":{"id":"5MQiaa6HuuRw"}},{"cell_type":"markdown","source":["This code defines a function `clustering_score(input_data, k)` to calculate the training cost of K-means clustering for a given value of `k` clusters. Within the function, it drops non-numeric columns, sets up a pipeline with a `VectorAssembler` and a K-means model, fits the pipeline to the data, and retrieves the training cost from the K-means model summary.\n","\n","Then, it iterates over a range of `k` values from 20 to 80 (inclusive), incrementing by 20, and prints the training cost for each `k`.\n"],"metadata":{"id":"e-UnDeDFGggx"}},{"cell_type":"code","source":["from pyspark.sql import DataFrame\n","from random import randint\n","\n","def clustering_score(input_data, k):\n","  input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n","  assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n","  kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n","  pipeline = Pipeline().setStages([assembler, kmeans])\n","  pipeline_model = pipeline.fit(input_numeric_only)\n","  kmeans_model = pipeline_model.stages[-1]\n","  training_cost = kmeans_model.summary.trainingCost\n","  return training_cost\n","\n","for k in list(range(20,100, 20)):\n","  print(clustering_score(numeric_only, k))"],"metadata":{"id":"i4puBGqKuvfS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code defines a modified version of the `clustering_score` function called `clustering_score_1`. It calculates the training cost of K-means clustering for a given value of `k` clusters. Within the function, it drops non-numeric columns, sets up a pipeline with a `VectorAssembler` and a K-means model with additional parameters (`setMaxIter`, `setTol`), and fits the pipeline to the data.\n","\n","Then, it iterates over a range of `k` values from 20 to 100 (inclusive), incrementing by 20, and prints the `k` value along with the corresponding training cost calculated by the `clustering_score_1` function.\n"],"metadata":{"id":"xaitmkRaGkCV"}},{"cell_type":"code","source":["def clustering_score_1(input_data, k):\n","  input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n","  assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n","  kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n","  pipeline = Pipeline().setStages([assembler, kmeans])\n","  pipeline_model = pipeline.fit(input_numeric_only)\n","  kmeans_model = pipeline_model.stages[-1]\n","  training_cost = kmeans_model.summary.trainingCost\n","  return training_cost\n","\n","for k in list(range(20,101, 20)):\n","  print(k, clustering_score_1(numeric_only, k))"],"metadata":{"id":"xK7PB96hu0vB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.3 Feature Normalization"],"metadata":{"id":"zKDLp_Y2u8Sl"}},{"cell_type":"markdown","source":["This code defines another modified version of the `clustering_score` function called `clustering_score_2`. It calculates the training cost of K-means clustering for a given value of `k` clusters. Within the function, it drops non-numeric columns, sets up a pipeline with a `VectorAssembler`, a `StandardScaler`, and a K-means model with additional parameters (`setMaxIter`, `setTol`), and fits the pipeline to the data.\n","\n","Then, it iterates over a range of `k` values from 60 to 270 (inclusive), incrementing by 30, and prints the `k` value along with the corresponding training cost calculated by the `clustering_score_2` function.\n"],"metadata":{"id":"hcbXTfm-Gnpq"}},{"cell_type":"code","source":["from pyspark.ml.feature import StandardScaler\n","\n","def clustering_score_2(input_data, k):\n","  input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n","  assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n","  scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n","  kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n","  pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n","  pipeline_model = pipeline.fit(input_numeric_only)\n","  kmeans_model = pipeline_model.stages[-1]\n","  training_cost = kmeans_model.summary.trainingCost\n","  return training_cost\n","\n","for k in list(range(60, 271, 30)):\n","  print(k, clustering_score_2(numeric_only, k))"],"metadata":{"id":"pMkNmsNqu9kq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.4 Categorical Variables"],"metadata":{"id":"IgMNVDw-vFkp"}},{"cell_type":"markdown","source":["This code defines a function `one_hot_pipeline(input_col)` to create a pipeline for one-hot encoding a categorical column. Within the function, it sets up a `StringIndexer` to index the categorical column and a `OneHotEncoder` to encode the indexed column. These transformations are encapsulated in a pipeline.\n","\n","The function returns the pipeline and the name of the output column after one-hot encoding.\n"],"metadata":{"id":"nQHAqaQ-Gry5"}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder, StringIndexer\n","\n","def one_hot_pipeline(input_col):\n","  indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col + \"_indexed\")\n","  encoder = OneHotEncoder().setInputCol(input_col + \"_indexed\").setOutputCol(input_col + \"_vec\")\n","  pipeline = Pipeline().setStages([indexer, encoder])\n","  return pipeline, input_col + \"_vec\""],"metadata":{"id":"-1U26-ShvGpq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code defines the function `clustering_score_3(input_data, k)` to calculate the training cost of K-means clustering for a given value of `k` clusters, considering one-hot encoded categorical features. Within the function, it sets up pipelines for one-hot encoding of categorical columns (\"protocol_type\", \"service\", \"flag\"), assembles all relevant columns, applies standard scaling, and trains a K-means model.\n","\n","Then, it iterates over a range of `k` values from 60 to 270 (inclusive), incrementing by 30, and prints the `k` value along with the corresponding training cost calculated by the `clustering_score_3` function.\n"],"metadata":{"id":"jfETwcu7GvE0"}},{"cell_type":"code","source":["def clustering_score_3(input_data, k):\n","  proto_type_pipeline, proto_type_vec_col = one_hot_pipeline(\"protocol_type\")\n","  service_pipeline, service_vec_col = one_hot_pipeline(\"service\")\n","  flag_pipeline, flag_vec_col = one_hot_pipeline(\"flag\")\n","\n","  assemble_cols = set(input_data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n","\n","  assembler = VectorAssembler().setInputCols(list(assemble_cols)).setOutputCol(\"featureVector\")\n","  scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n","  kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"scaledFeatureVector\")\n","  pipeline = Pipeline().setStages([proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n","  pipeline_model = pipeline.fit(input_data)\n","\n","  kmeans_model = pipeline_model.stages[-1]\n","  training_cost = kmeans_model.summary.trainingCost\n","  return training_cost\n","\n","for k in list(range(60, 271, 30)):\n","  print(k, clustering_score_3(data, k))"],"metadata":{"id":"8PYT2zS6vM3m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.5 Using Labels with Entropy"],"metadata":{"id":"4_BFiGD6vUBO"}},{"cell_type":"markdown","source":["This code defines a function `entropy(counts)` to calculate the entropy of a list of counts. Within the function, it calculates the probability of each count, computes the entropy using the formula `-p * log(p)`, and sums the results. The function returns the entropy value.\n"],"metadata":{"id":"PoPm_uHrGyr-"}},{"cell_type":"code","source":["from math import log\n","\n","def entropy(counts):\n","  values = [c for c in counts if (c > 0)]\n","  n = sum(values)\n","  p = [v/n for v in values]\n","  return sum([-1*(p_v) * log(p_v) for p_v in p])"],"metadata":{"id":"Qf99H0_JvQYS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code computes the weighted average of cluster entropy for the provided data. It first transforms the data using the trained pipeline model to obtain cluster labels. Then, it calculates the count of each label within each cluster and orders the result by cluster. After that, it computes the probability of each label count within each cluster and adds a new column for the probability.\n","\n","Next, it calculates the entropy for each cluster by summing the product of the probability and the log probability for each label count within the cluster. Then, it computes the weighted cluster entropy by multiplying the entropy with the cluster size.\n","\n","Finally, it aggregates the weighted cluster entropy across all clusters and divides it by the total number of data points to get the weighted average cluster entropy.\n"],"metadata":{"id":"wgce4T3SG3S5"}},{"cell_type":"code","source":["from pyspark.sql import functions as fun\n","from pyspark.sql import Window\n","\n","cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\")\n","\n","df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n","\n","w = Window.partitionBy(\"cluster\")\n","\n","p_col = df['count'] / fun.sum(df['count']).over(w)\n","with_p_col = df.withColumn(\"p_col\", p_col)\n","\n","result = with_p_col.groupBy(\"cluster\").agg((-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\")))).alias(\"entropy\"),fun.sum(col(\"count\")).alias(\"cluster_size\"))\n","\n","result = result.withColumn('weightedClusterEntropy',fun.col('entropy') * fun.col('cluster_size'))\n","\n","weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n","weighted_cluster_entropy_avg[0][0]/data.count()"],"metadata":{"id":"hDfGXqkQvWF2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code defines a function `fit_pipeline_4(data, k)` to fit a pipeline for K-means clustering with one-hot encoding of categorical features and standard scaling. It first sets up pipelines for one-hot encoding of categorical columns (\"protocol_type\", \"service\", \"flag\"). Then, it assembles relevant columns, applies standard scaling, and trains a K-means model using the specified number of clusters (`k`).\n","\n","Another function `clustering_score_4(input_data, k)` is defined to calculate the weighted average cluster entropy for a given dataset and number of clusters. Within the function, it fits the pipeline to the input data, transforms the data to obtain cluster labels, calculates the cluster entropy, and computes the weighted average cluster entropy.\n","\n","Both functions are designed to enhance modularity and readability, making it easier to fit the pipeline and calculate the clustering score.\n"],"metadata":{"id":"fMWTphFHG6pu"}},{"cell_type":"code","source":["def fit_pipeline_4(data, k):\n","  (proto_type_pipeline, proto_type_vec_col) = one_hot_pipeline(\"protocol_type\")\n","  (service_pipeline, service_vec_col) = one_hot_pipeline(\"service\")\n","  (flag_pipeline, flag_vec_col) = one_hot_pipeline(\"flag\")\n","  assemble_cols = set(data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n","  assembler = VectorAssembler(inputCols=list(assemble_cols), outputCol=\"featureVector\")\n","\n","  scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n","\n","  kmeans = KMeans(seed=randint(100, 100000), k=k, predictionCol=\"cluster\", featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-5)\n","\n","  pipeline = Pipeline(stages=[proto_type_pipeline, service_pipeline, flag_pipeline, assembler, scaler, kmeans])\n","  return pipeline.fit(data)\n","\n","def clustering_score_4(input_data, k):\n","  pipeline_model = fit_pipeline_4(input_data, k)\n","  cluster_label = pipeline_model.transform(input_data).select(\"cluster\", \"label\")\n","\n","  df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n","\n","  w = Window.partitionBy(\"cluster\")\n","\n","  p_col = df['count'] / fun.sum(df['count']).over(w)\n","  with_p_col = df.withColumn(\"p_col\", p_col)\n","\n","  result = with_p_col.groupBy(\"cluster\").agg(-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\"))).alias(\"entropy\"),fun.sum(col(\"count\")).alias(\"cluster_size\"))\n","\n","  result = result.withColumn('weightedClusterEntropy', col('entropy') * col('cluster_size'))\n","\n","  weighted_cluster_entropy_avg = result.agg(fun.sum(col('weightedClusterEntropy'))).collect()\n","  return weighted_cluster_entropy_avg[0][0] / input_data.count()"],"metadata":{"id":"J56V_FyqvZqq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.6 Clustering in Action"],"metadata":{"id":"fZ8Q12BHvjRm"}},{"cell_type":"markdown","source":["This code fits the pipeline model for K-means clustering with 180 clusters to the provided data and then applies the trained model to the data to obtain cluster labels. It selects the 'cluster' and 'label' columns, groups them by cluster and label, calculates the count of each combination, and orders the result by cluster and label. Finally, it displays the resulting DataFrame showing the counts of each label within each cluster.\n"],"metadata":{"id":"jovUceeaG_hR"}},{"cell_type":"code","source":["pipeline_model = fit_pipeline_4(data, 180)\n","count_by_cluster_label = pipeline_model.transform(data).select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\", \"label\")\n","count_by_cluster_label.show()"],"metadata":{"id":"zrCUQ2f_vkdz"},"execution_count":null,"outputs":[]}]}