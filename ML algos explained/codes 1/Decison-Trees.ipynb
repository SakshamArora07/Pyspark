{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNm5E7TFO5vm0x9Rj9G2e9W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Setting Up PySpark Environment\n","\n","This code snippet sets up the PySpark environment.\n","\n","- **Importing Libraries**: Necessary libraries such as `pyspark`, `os`, and `sys` are imported.\n","- **Setting Python Executable**: The Python executable path is set for both worker nodes and the driver.\n","- **Setting Spark Context**: A SparkContext is created using `SparkContext` from `pyspark`.\n","- **Setting SparkSession**: A SparkSession is created using `SparkSession` from `pyspark.sql`.\n","\n","Setting up the PySpark environment is essential for initializing SparkContext and SparkSession, enabling interaction with Spark clusters and distributed computing.\n"],"metadata":{"id":"hsj1ZPGX3TTk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7dfgWOYzSLg"},"outputs":[],"source":["import pyspark\n","import os\n","import sys\n","from pyspark import SparkContext\n","os.environ['PYSPARK_PYTHON'] = sys.executable\n","os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n","\n","from pyspark.sql import SparkSession"]},{"cell_type":"markdown","source":["## Configuring Spark Session\n","\n","This code cell configures a SparkSession named 'chapter_4' with specific driver memory settings.\n","\n","- **Configuring Driver Memory**: `.config(\"spark.driver.memory\", \"16g\")` sets the driver memory to 16 gigabytes.\n","- **Creating SparkSession**: `SparkSession.builder.appName('chapter_4').getOrCreate()` creates a SparkSession with the specified configuration and application name.\n","\n","Configuring the SparkSession with appropriate memory settings is crucial for managing resources effectively and optimizing performance, especially for memory-intensive tasks.\n"],"metadata":{"id":"eHADzew73Xrp"}},{"cell_type":"code","source":["spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_4').getOrCreate()"],"metadata":{"id":"Fodx0sSuzb5K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.1 Preparing the Data"],"metadata":{"id":"CIYiWNq8zddr"}},{"cell_type":"markdown","source":["## Reading CSV Data Without Header\n","\n","This code cell reads CSV data from the file path \"data/covtype.data\" without inferring the schema and without considering the first row as the header.\n","\n","- **CSV File Path**: The CSV data is read from the file path \"data/covtype.data\".\n","- **Parsing Options**:\n","  - `inferSchema=True`: Specifies to infer the schema automatically.\n","  - `header=False`: Specifies that the first row should not be considered as the header.\n","- **DataFrame Creation**: The resulting DataFrame contains the CSV data without header and with inferred schema.\n","\n","This operation reads the CSV data into a DataFrame without considering the first row as the header, allowing for manual specification of the schema if needed.\n"],"metadata":{"id":"rYkEvjA63cXl"}},{"cell_type":"code","source":["data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"data/covtype.data\")\n","data_without_header.printSchema()"],"metadata":{"id":"AaFHiHd2zfBl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Specifying Column Names and Data Types\n","\n","This code cell specifies column names and data types for the DataFrame `data_without_header` and converts the \"Cover_Type\" column to DoubleType.\n","\n","- **Column Names**: A list `colnames` containing column names is defined.\n","- **Data Types**: The \"Cover_Type\" column is casted to DoubleType using `.cast(DoubleType())`.\n","- **DataFrame Transformation**: The DataFrame `data_without_header` is transformed by specifying column names and converting data types.\n","- **Head**: `.head()` is used to display the first row of the transformed DataFrame.\n","\n","This transformation ensures that the DataFrame `data` has meaningful column names and correct data types, facilitating further analysis and processing.\n"],"metadata":{"id":"vC0fQIjv3odW"}},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\n","from pyspark.sql.functions import col\n","\n","colnames = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \"Horizontal_Distance_To_Fire_Points\"] + [f\"Wilderness_Area_{i}\" for i in range(4)] + [f\"Soil_Type_{i}\" for i in range(40)] + [\"Cover_Type\"]\n","\n","data = data_without_header.toDF(*colnames).withColumn(\"Cover_Type\", col(\"Cover_Type\").cast(DoubleType()))\n","\n","data.head()"],"metadata":{"id":"xlif53oIzg_A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.2 Our First Decision Tree"],"metadata":{"id":"tZIN2wCzzkhy"}},{"cell_type":"markdown","source":["## Splitting Data into Training and Testing Sets\n","\n","This code cell splits the DataFrame `data` into training and testing sets.\n","\n","- **Random Splitting**: `data.randomSplit([0.9, 0.1])` randomly splits the data into two sets with a 90:10 ratio for training and testing, respectively.\n","- **Caching**: Both the training and testing sets (`train_data` and `test_data`) are cached for faster access.\n","\n","- **Training Set**: `train_data` contains 90% of the data and is cached.\n","- **Testing Set**: `test_data` contains 10% of the data and is cached.\n","\n","Splitting the data into training and testing sets is essential for evaluating the performance of machine learning models and preventing overfitting.\n"],"metadata":{"id":"UnfAhYRh3rEx"}},{"cell_type":"code","source":["(train_data, test_data) = data.randomSplit([0.9, 0.1])\n","train_data.cache()\n","test_data.cache()"],"metadata":{"id":"pIukN3NozlsF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assembling Feature Vectors\n","\n","This code cell assembles feature vectors using the `VectorAssembler` from PySpark.\n","\n","- **Input Columns**: The variable `input_cols` contains all column names except the last one, which is the target variable.\n","- **VectorAssembler**: A `VectorAssembler` named `vector_assembler` is created with input columns specified by `inputCols` and output column specified by `outputCol`.\n","- **Transforming Data**: `vector_assembler.transform(train_data)` applies the `VectorAssembler` to the training data, creating a new column named \"featureVector\" containing the assembled feature vectors.\n","- **Displaying Feature Vectors**: `assembled_train_data.select(\"featureVector\").show(truncate=False)` displays the assembled feature vectors.\n","\n","Assembling feature vectors is a common preprocessing step in machine learning pipelines, combining multiple features into a single vector format suitable for model training.\n"],"metadata":{"id":"4pugpSgw3u9z"}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","\n","input_cols = colnames[:-1]\n","vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n","\n","assembled_train_data = vector_assembler.transform(train_data)\n","\n","assembled_train_data.select(\"featureVector\").show(truncate = False)"],"metadata":{"id":"pUEz2FwkznbW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training Decision Tree Classifier\n","\n","This code cell trains a decision tree classifier using the `DecisionTreeClassifier` from PySpark MLlib.\n","\n","- **Creating Classifier**: A `DecisionTreeClassifier` named `classifier` is created with parameters such as seed, label column, features column, and prediction column.\n","- **Model Training**: `classifier.fit(assembled_train_data)` trains the decision tree classifier on the assembled training data.\n","- **Printing Model Debug String**: `model.toDebugString` prints the debug string representation of the trained decision tree model.\n","\n","The debug string provides a human-readable representation of the decision tree model, showing the decision rules and splits made by the tree at each node.\n"],"metadata":{"id":"Jr82lm8a3zgm"}},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n","\n","classifier = DecisionTreeClassifier(seed = 1234, labelCol=\"Cover_Type\", featuresCol=\"featureVector\", predictionCol=\"prediction\")\n","\n","model = classifier.fit(assembled_train_data)\n","print(model.toDebugString)"],"metadata":{"id":"m6D7hlYAzrby"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extracting Feature Importances\n","\n","This code cell extracts feature importances from the trained decision tree model and presents them in a Pandas DataFrame.\n","\n","- **Importing Libraries**: The `pandas` library is imported as `pd`.\n","- **Converting Feature Importances**: `model.featureImportances.toArray()` converts the feature importances from the trained model to a NumPy array.\n","- **Creating DataFrame**: `pd.DataFrame(...)` creates a DataFrame with feature importances, using input column names as index and \"importance\" as the column name.\n","- **Sorting by Importance**: `.sort_values(by=\"importance\", ascending=False)` sorts the DataFrame by importance values in descending order.\n","\n","This DataFrame provides insights into the importance of each feature in predicting the target variable, helping to understand the significance of different features in the decision-making process of the model.\n"],"metadata":{"id":"JFhEn68J34AT"}},{"cell_type":"code","source":["import pandas as pd\n","\n","pd.DataFrame(model.featureImportances.toArray(), index=input_cols, columns=['importance']).sort_values(by=\"importance\", ascending=False)"],"metadata":{"id":"J5UoYqAVztJA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Making Predictions\n","\n","This code cell makes predictions using the trained decision tree model on the assembled training data.\n","\n","- **Model Transformation**: `model.transform(assembled_train_data)` applies the trained decision tree model to the assembled training data, generating predictions.\n","- **Selecting Columns**: `.select(\"Cover_Type\", \"prediction\", \"probability\")` selects the columns for target variable, predicted label, and probability distribution of each class.\n","- **Displaying Predictions**: `.show(10, truncate=False)` displays the first 10 rows of the DataFrame containing predictions.\n","\n","This output provides a glimpse of the actual target variable, predicted label, and probability distribution for each class, aiding in assessing the performance of the trained model.\n"],"metadata":{"id":"bVjyDvet38AG"}},{"cell_type":"code","source":["predictions = model.transform(assembled_train_data)\n","predictions.select(\"Cover_Type\", \"prediction\", \"probability\").show(10, truncate = False)"],"metadata":{"id":"4bIgIKnLzv42"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluating Model Performance\n","\n","This code cell evaluates the performance of the trained decision tree model using the `MulticlassClassificationEvaluator` from PySpark MLlib.\n","\n","- **Creating Evaluator**: A `MulticlassClassificationEvaluator` named `evaluator` is created with parameters specifying the label column and prediction column.\n","- **Accuracy Evaluation**: `.setMetricName(\"accuracy\").evaluate(predictions)` evaluates the accuracy of the predictions made by the model.\n","- **F1 Score Evaluation**: `.setMetricName(\"f1\").evaluate(predictions)` evaluates the F1 score of the predictions made by the model.\n","\n","Evaluating model performance is crucial for assessing its effectiveness in making accurate predictions and identifying areas for improvement.\n"],"metadata":{"id":"M9MFRcHl4AjI"}},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\", predictionCol=\"prediction\")\n","\n","evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n","evaluator.setMetricName(\"f1\").evaluate(predictions)"],"metadata":{"id":"FMOVwK73zyMS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Computing Confusion Matrix\n","\n","This code cell computes the confusion matrix based on the predictions made by the trained model.\n","\n","- **Grouping and Pivot**: `predictions.groupBy(\"Cover_Type\").pivot(\"prediction\", range(1,8)).count()` groups the data by the actual cover type and pivots the predicted cover types, counting the occurrences for each combination.\n","- **Handling Missing Values**: `.na.fill(0.0)` fills missing values with zeros.\n","- **Ordering by Cover Type**: `.orderBy(\"Cover_Type\")` orders the confusion matrix by the actual cover type.\n","\n","The resulting confusion matrix provides insights into the model's performance by showing how often each actual cover type was predicted as each possible cover type.\n"],"metadata":{"id":"nd-CTMPi4EU7"}},{"cell_type":"code","source":["confusion_matrix = predictions.groupBy(\"Cover_Type\").pivot(\"prediction\", range(1,8)).count().na.fill(0.0).orderBy(\"Cover_Type\")\n","\n","confusion_matrix.show()"],"metadata":{"id":"5zJK5kQkzzdc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Computing Class Probabilities\n","\n","This code defines a function `class_probabilities` to compute class probabilities from a given DataFrame.\n","\n","- **Function Definition**: The function takes a DataFrame `data` as input and computes the proportion of each class in the data.\n","- **Total Count**: The total count of data points is computed using `data.count()`.\n","- **Grouping and Counting**: `data.groupBy(\"Cover_Type\").count()` groups the data by cover type and counts the occurrences of each cover type.\n","- **Calculating Proportions**: The count for each cover type is divided by the total count to calculate the proportion.\n","- **Collecting Results**: The computed proportions are collected into a list.\n","\n","These class probabilities are essential for understanding the distribution of classes in the data, which can influence model training and evaluation.\n"],"metadata":{"id":"VTbBfHfE4JMN"}},{"cell_type":"code","source":["from pyspark.sql import DataFrame\n","\n","def class_probabilities(data):\n","  total = data.count()\n","  return data.groupBy(\"Cover_Type\").count().orderBy(\"Cover_Type\").select(col(\"count\").cast(DoubleType())).withColumn(\"count_proportion\", col(\"count\")/total).select(\"count_proportion\").collect()\n","\n","train_prior_probabilities = class_probabilities(train_data)\n","test_prior_probabilities = class_probabilities(test_data)\n","train_prior_probabilities"],"metadata":{"id":"nggp5_2sz1YT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Computing Weighted Average\n","\n","This code cell computes the weighted average of prior probabilities using the train and test data.\n","\n","- **Extracting Values**: `[p[0] for p in train_prior_probabilities]` and `[p[0] for p in test_prior_probabilities]` extract the probability values from the lists.\n","- **Weighted Average Calculation**: `sum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities,test_prior_probabilities)])` calculates the weighted average by multiplying corresponding probabilities from train and test data and summing the results.\n","\n","Weighted average of prior probabilities provides a measure of the expected prior probability, considering the distribution of classes in both train and test data.\n"],"metadata":{"id":"F3JMCBHC4Ou4"}},{"cell_type":"code","source":["train_prior_probabilities = [p[0] for p in train_prior_probabilities]\n","test_prior_probabilities = [p[0] for p in test_prior_probabilities]\n","\n","sum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities,test_prior_probabilities)])"],"metadata":{"id":"qz4t6DEmz4sY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.3 Tuning Decision Trees"],"metadata":{"id":"Me_lx6ujz6Kc"}},{"cell_type":"markdown","source":["## Creating ML Pipeline\n","\n","This code cell creates a machine learning pipeline using the `Pipeline` class from PySpark MLlib.\n","\n","- **Feature Assembler**: A `VectorAssembler` named `assembler` is created to assemble feature vectors from input columns.\n","- **Decision Tree Classifier**: A `DecisionTreeClassifier` named `classifier` is created with specified parameters.\n","- **Pipeline Stages**: Both `assembler` and `classifier` are included as stages in the pipeline.\n","- **Pipeline Creation**: `Pipeline(stages=[assembler, classifier])` creates a pipeline with the specified stages.\n","\n","Machine learning pipelines are useful for chaining together multiple stages of data processing and model training, providing a unified interface for model development and deployment.\n"],"metadata":{"id":"avfl6vGD4S-Q"}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n","\n","assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n","classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\",featuresCol=\"featureVector\",predictionCol=\"prediction\")\n","\n","pipeline = Pipeline(stages=[assembler, classifier])"],"metadata":{"id":"9e0ozVf_z7SV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Building Parameter Grid for Model Tuning\n","\n","This code cell builds a parameter grid for tuning the decision tree classifier using the `ParamGridBuilder` from PySpark MLlib.\n","\n","- **Parameter Grid Building**: `ParamGridBuilder()` initializes a parameter grid builder.\n","- **Adding Grids**: `.addGrid(...)` adds grids for different hyperparameters of the decision tree classifier, such as impurity, max depth, max bins, and min info gain.\n","- **Building Parameter Grid**: `.build()` builds the parameter grid containing all combinations of specified hyperparameters.\n","\n","Parameter grid building is a crucial step in hyperparameter tuning, allowing for systematic exploration of different parameter combinations to find the optimal model configuration.\n"],"metadata":{"id":"fjUXasZK4XCm"}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\n","\n","paramGrid = ParamGridBuilder().addGrid(classifier.impurity, [\"gini\", \"entropy\"]).addGrid(classifier.maxDepth, [1, 20]).addGrid(classifier.maxBins, [40, 300]).addGrid(classifier.minInfoGain, [0.0, 0.05]).build()\n","\n","multiclassEval = MulticlassClassificationEvaluator().setLabelCol(\"Cover_Type\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")"],"metadata":{"id":"dEBXCURez858"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training and Validation Split\n","\n","This code cell performs training and validation split using the `TrainValidationSplit` class from PySpark MLlib.\n","\n","- **Setting Up Validator**: A `TrainValidationSplit` named `validator` is created with specified parameters including the pipeline (estimator), evaluator, parameter grid, and train ratio.\n","- **Model Training**: `validator.fit(train_data)` trains the validator on the training data.\n","\n","Training and validation split is a common technique used in machine learning for evaluating model performance and tuning hyperparameters, helping to prevent overfitting and ensure generalization to unseen data.\n"],"metadata":{"id":"j-S0tm6s4cIs"}},{"cell_type":"code","source":["from pyspark.ml.tuning import TrainValidationSplit\n","\n","validator = TrainValidationSplit(seed=1234,estimator=pipeline,evaluator=multiclassEval,estimatorParamMaps=paramGrid,trainRatio=0.9)\n","\n","validator_model = validator.fit(train_data)"],"metadata":{"id":"eOI8pWBYz-9Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extracting Best Model Parameters\n","\n","This code cell extracts the parameters of the best model obtained from the validation process.\n","\n","- **Accessing Best Model**: `validator_model.bestModel` retrieves the best model selected during the validation process.\n","- **Extracting Parameter Map**: `.stages[1].extractParamMap()` extracts the parameter map of the decision tree classifier stage from the best model.\n","\n","Extracting the parameters of the best model allows for understanding the configuration that yielded the best performance, aiding in model interpretation and further optimization.\n"],"metadata":{"id":"zoxw0kDA4guT"}},{"cell_type":"code","source":["from pprint import pprint\n","\n","best_model = validator_model.bestModel\n","pprint(best_model.stages[1].extractParamMap())"],"metadata":{"id":"SuNKQOW20BgW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Re-training and Extracting Validation Metrics and Parameters\n","\n","This code cell re-trains the validator on the training data and extracts the validation metrics along with the corresponding parameter maps.\n","\n","- **Re-training Validator**: `validator.fit(train_data)` re-trains the validator on the training data.\n","- **Extracting Metrics and Parameters**: `validator_model.validationMetrics` retrieves the validation metrics, and `validator_model.getEstimatorParamMaps()` retrieves the corresponding parameter maps.\n","\n","The validation metrics and parameter maps are then combined into a list and sorted based on the validation metrics in descending order.\n","\n","Re-training the validator and examining the validation metrics and parameters helps in understanding the performance of different hyperparameter combinations and selecting the best model configuration.\n"],"metadata":{"id":"EcIzi-PF4kTg"}},{"cell_type":"code","source":["validator_model = validator.fit(train_data)\n","\n","metrics = validator_model.validationMetrics\n","params = validator_model.getEstimatorParamMaps()\n","metrics_and_params = list(zip(metrics, params))\n","\n","metrics_and_params.sort(key=lambda x: x[0], reverse=True)\n","metrics_and_params"],"metadata":{"id":"AhVFhbDN0DcI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Printing Highest Metric Value\n","\n","This code snippet sorts the `metrics` list in descending order and prints the highest value.\n","\n","- **Sorting Metrics**: `metrics.sort(reverse=True)` sorts the list of metrics in descending order.\n","- **Printing Highest Value**: `print(metrics[0])` prints the highest value from the sorted list.\n","\n","This operation helps identify the highest metric value obtained during the validation process, providing insights into the performance of the model configurations.\n"],"metadata":{"id":"heEJabFz4oPI"}},{"cell_type":"code","source":["metrics.sort(reverse=True)\n","print(metrics[0])"],"metadata":{"id":"RMNpVkFm0GI9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluating Model Performance on Test Data\n","\n","This code snippet evaluates the performance of the best model obtained from the validation process on the test data.\n","\n","- **Model Transformation**: `best_model.transform(test_data)` applies the best model to the test data, generating predictions.\n","- **Evaluation**: `multiclassEval.evaluate(...)` evaluates the performance of the predictions using the specified evaluator (`multiclassEval`).\n","\n","Evaluating the model on the test data provides an assessment of its generalization ability and helps determine its effectiveness in making predictions on unseen data.\n"],"metadata":{"id":"v4K2QfuX40sf"}},{"cell_type":"code","source":["multiclassEval.evaluate(best_model.transform(test_data))"],"metadata":{"id":"loJr8gmH0Hfl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.4 Categorical Features Revisited"],"metadata":{"id":"8BVBU_bs0I-q"}},{"cell_type":"markdown","source":["## One-Hot Encoding Reverse Transformation\n","\n","This code defines a function `unencode_one_hot` to reverse the one-hot encoding applied to categorical features.\n","\n","- **Wilderness Area Columns**: Wilderness area columns are specified as `wilderness_cols`.\n","- **Wilderness Area Assembler**: A `VectorAssembler` is created to assemble the wilderness area columns into a single vector column named \"wilderness\".\n","- **Soil Type Columns**: Soil type columns are specified as `soil_cols`.\n","- **Soil Type Assembler**: A `VectorAssembler` is created to assemble the soil type columns into a single vector column named \"soil\".\n","- **Reverse Encoding UDF**: A user-defined function (UDF) is defined to find the index of the non-zero value in a vector, effectively reversing the one-hot encoding.\n","- **Applying Transformations**: The wilderness and soil type columns are transformed using the UDF to obtain the original categorical values.\n","- **Data Transformation**: The function returns the DataFrame with the one-hot encoded columns reversed.\n","\n","Reverse transformation of one-hot encoding is essential for interpreting and analyzing categorical features in their original form.\n"],"metadata":{"id":"sFlMQqte44wq"}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import IntegerType\n","\n","def unencode_one_hot(data):\n","  wilderness_cols = ['Wilderness_Area_' + str(i) for i in range(4)]\n","  wilderness_assembler = VectorAssembler().setInputCols(wilderness_cols).setOutputCol(\"wilderness\")\n","\n","  unhot_udf = udf(lambda v: v.toArray().tolist().index(1))\n","\n","  with_wilderness = wilderness_assembler.transform(data).drop(*wilderness_cols).withColumn(\"wilderness\", unhot_udf(col(\"wilderness\")).cast(IntegerType()))\n","\n","  soil_cols = ['Soil_Type_' + str(i) for i in range(40)]\n","  soil_assembler = VectorAssembler().setInputCols(soil_cols).setOutputCol(\"soil\")\n","  with_soil = soil_assembler.transform(with_wilderness).drop(*soil_cols).withColumn(\"soil\", unhot_udf(col(\"soil\")).cast(IntegerType()))\n","\n","  return with_soil"],"metadata":{"id":"BLrfvy700KA9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Applying One-Hot Encoding Reverse Transformation\n","\n","This code cell applies the reverse transformation function `unencode_one_hot` to the training data.\n","\n","- **Function Application**: `unencode_one_hot(train_data)` applies the reverse transformation function to the training data.\n","- **Printing Schema**: `.printSchema()` displays the schema of the transformed DataFrame `unenc_train_data`.\n","\n","This operation reverses the one-hot encoding applied to the categorical features in the training data, allowing for analysis and interpretation of the original categorical values.\n"],"metadata":{"id":"FzwJZKtT49g3"}},{"cell_type":"code","source":["unenc_train_data = unencode_one_hot(train_data)\n","unenc_train_data.printSchema()"],"metadata":{"id":"4TK31dDH0M3D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Grouping by Reversed One-Hot Encoded Feature\n","\n","This code cell groups the DataFrame `unenc_train_data` by the reversed one-hot encoded feature \"wilderness\" and counts the occurrences of each category.\n","\n","- **Grouping by Feature**: `.groupBy('wilderness')` groups the data by the reversed one-hot encoded feature \"wilderness\".\n","- **Counting Occurrences**: `.count()` counts the occurrences of each category within the \"wilderness\" feature.\n","- **Displaying Results**: `.show()` displays the counts of each category.\n","\n","This operation helps to understand the distribution of categories within the reversed one-hot encoded feature \"wilderness\" in the training data.\n"],"metadata":{"id":"lzXr1Zrk5BuY"}},{"cell_type":"code","source":["unenc_train_data.groupBy('wilderness').count().show()"],"metadata":{"id":"x2yR8HIw0OcW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating ML Pipeline with Vector Indexer\n","\n","This code cell constructs a machine learning pipeline including a VectorIndexer stage.\n","\n","- **Input Columns**: The variable `input_cols` contains all columns except the target variable \"Cover_Type\".\n","- **Feature Assembler**: A `VectorAssembler` is created to assemble the input columns into a single feature vector named \"featureVector\".\n","- **Vector Indexer**: A `VectorIndexer` stage is added to the pipeline to automatically identify categorical features and index them. It is configured to handle a maximum of 40 distinct values.\n","- **Decision Tree Classifier**: A `DecisionTreeClassifier` is added to the pipeline, configured with appropriate label and feature columns.\n","- **Pipeline Creation**: The pipeline is created and set with stages including the feature assembler, vector indexer, and classifier.\n","\n","Using VectorIndexer in the pipeline helps to automatically identify categorical features and index them appropriately, which can improve the performance of tree-based models.\n"],"metadata":{"id":"2wXX6WVg5IZ1"}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorIndexer\n","\n","cols = unenc_train_data.columns\n","input_cols = [c for c in cols if c!='Cover_Type']\n","\n","assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")\n","\n","indexer = VectorIndexer().setMaxCategories(40).setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n","\n","classifier = DecisionTreeClassifier().setLabelCol(\"Cover_Type\").setFeaturesCol(\"indexedVector\").setPredictionCol(\"prediction\")\n","\n","pipeline = Pipeline().setStages([assembler, indexer, classifier])"],"metadata":{"id":"MzavW08f0Q8I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.5 Random Forests Takes Too Long To Run"],"metadata":{"id":"ylmv_0tY0SHY"}},{"cell_type":"markdown","source":["## Updating Classifier to RandomForestClassifier\n","\n","This code cell updates the classifier in the pipeline to use a RandomForestClassifier instead of a DecisionTreeClassifier.\n","\n","- **Random Forest Classifier**: The `RandomForestClassifier` from PySpark MLlib is instantiated with specified parameters including seed, label column, features column, and prediction column.\n","- **Classifier Update**: The `classifier` variable is updated to use the RandomForestClassifier.\n","\n","Random forests are an ensemble learning method that builds multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. They are known for their robustness and ability to handle complex datasets.\n"],"metadata":{"id":"0mVUfQg35NF-"}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n","\n","classifier = RandomForestClassifier(seed=1234, labelCol=\"Cover_Type\",featuresCol=\"indexedVector\",predictionCol=\"prediction\")"],"metadata":{"id":"uga9bw090TR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unenc_train_data.columns"],"metadata":{"id":"9pxAMGtf0WL-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tuning Random Forest Classifier with TrainValidationSplit\n","\n","This code cell performs hyperparameter tuning for the Random Forest Classifier using TrainValidationSplit.\n","\n","- **Input Columns**: The variable `input_cols` contains all columns except the target variable \"Cover_Type\".\n","- **Feature Assembler**: A `VectorAssembler` is created to assemble the input columns into a single feature vector named \"featureVector\".\n","- **Vector Indexer**: A `VectorIndexer` stage is added to the pipeline to automatically identify categorical features and index them. It is configured to handle a maximum of 40 distinct values.\n","- **Random Forest Classifier**: The pipeline includes a `RandomForestClassifier` for classification tasks.\n","- **Parameter Grid Building**: A parameter grid is constructed with various hyperparameters for the Random Forest Classifier, such as impurity, max depth, max bins, and min info gain.\n","- **Multiclass Evaluator**: A `MulticlassClassificationEvaluator` is configured to evaluate the model's performance using accuracy.\n","- **Train Validation Split**: The `TrainValidationSplit` estimator is used for hyperparameter tuning, with the pipeline, evaluator, parameter grid, and train ratio specified.\n","- **Model Training**: The validator is fit to the training data, resulting in the selection of the best model configuration.\n"],"metadata":{"id":"jdNSVeCu5TVP"}},{"cell_type":"code","source":["######### LONGER TIME ##################################\n","\n","cols = unenc_train_data.columns\n","input_cols = [c for c in cols if c!='Cover_Type']\n","\n","assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")\n","\n","indexer = VectorIndexer().setMaxCategories(40).setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n","\n","pipeline = Pipeline().setStages([assembler, indexer, classifier])\n","\n","paramGrid = ParamGridBuilder(). \\\n","  addGrid(classifier.impurity, [\"gini\", \"entropy\"]). \\\n","  addGrid(classifier.maxDepth, [1, 20]). \\\n","  addGrid(classifier.maxBins, [40, 300]). \\\n","  addGrid(classifier.minInfoGain, [0.0, 0.05]). \\\n","  build()\n","\n","multiclassEval = MulticlassClassificationEvaluator(). \\\n","  setLabelCol(\"Cover_Type\"). \\\n","  setPredictionCol(\"prediction\"). \\\n","  setMetricName(\"accuracy\")\n","\n","validator = TrainValidationSplit(seed=1234,\n","  estimator=pipeline,\n","  evaluator=multiclassEval,\n","  estimatorParamMaps=paramGrid,\n","  trainRatio=0.9)\n","\n","validator_model = validator.fit(unenc_train_data)\n","\n","best_model = validator_model.bestModel"],"metadata":{"id":"0qWseMMY0YxF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extracting Feature Importance from Random Forest Model\n","\n","This code cell extracts the feature importance from the trained Random Forest model.\n","\n","- **Accessing Forest Model**: `best_model.stages[2]` retrieves the Random Forest model from the best model obtained from the validation process.\n","- **Feature Importance Extraction**: `.featureImportances.toArray()` extracts the feature importances as an array.\n","- **Zip and Sort**: `zip(input_cols, forest_model.featureImportances.toArray())` zips the input column names with their corresponding feature importances and sorts them based on importance in descending order.\n","- **Printing Feature Importance**: The sorted list of feature importances is printed.\n","\n","Understanding feature importance is crucial for identifying the most influential features in the model's decision-making process, aiding in feature selection and interpretation.\n"],"metadata":{"id":"GIihqRs55Y0C"}},{"cell_type":"code","source":["forest_model = best_model.stages[2]\n","\n","feature_importance_list = list(zip(input_cols,forest_model.featureImportances.toArray()))\n","\n","feature_importance_list.sort(key=lambda x: x[1], reverse=True)\n","\n","pprint(feature_importance_list)"],"metadata":{"id":"EVcmNwkd0cfX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0.0.6 Making Predictions"],"metadata":{"id":"1tSpmh0S0fL9"}},{"cell_type":"markdown","source":["## Making Predictions on Test Data with Best Model\n","\n","This code cell makes predictions on the test data using the best model obtained from the validation process.\n","\n","- **One-Hot Encoding Reverse Transformation**: The test data is first transformed using the `unencode_one_hot` function to reverse the one-hot encoding applied to categorical features.\n","- **Dropping Target Column**: The target column \"Cover_Type\" is dropped from the transformed test data.\n","- **Model Prediction**: `best_model.transform(...)` applies the best model to the transformed test data, generating predictions.\n","- **Selecting Prediction Column**: `.select(\"prediction\")` selects the prediction column from the output.\n","- **Displaying Predictions**: `.show(1)` displays the predictions for one data point from the test data.\n","\n","This operation allows for assessing the model's performance on unseen test data by examining its predictions.\n"],"metadata":{"id":"RN703ZFG5dO-"}},{"cell_type":"code","source":["unenc_test_data = unencode_one_hot(test_data)\n","\n","best_model.transform(unenc_test_data.drop(\"Cover_Type\")).select(\"prediction\").show(1)"],"metadata":{"id":"YWG5T2tD0gTF"},"execution_count":null,"outputs":[]}]}